---
title: "07. Vanishing Gradients and Fancy RNNs"
metaTitle: "This is the title tag of this page"
metaDescription: "This is the meta description"
---


Today weâ€™ll learn:
- Problems with RNNs and how to fix them 
- More complex RNN variants

Vanishing gradient problem -motivates->
- Two new types of RNN: LSTM and GRU

Other fixes for vanishing (or exploding) gradient:
- Gradient clipping 
- Skip connections

More fancy RNN variants: 
- Bidirectional RNNs
- Multi-layer RNNs


WIP: Comeback after doing lectures 3 and 4
