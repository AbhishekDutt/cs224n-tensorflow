{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1TqvsPFEk5w5+xpu8HKhq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhishekDutt/cs224n-tensorflow/blob/master/A3/A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpUF038TlizD",
        "colab_type": "text"
      },
      "source": [
        "Testing Latex in MD:\n",
        "\n",
        "$$c = \\sqrt{a^2 + b^2}$$\n",
        "\n",
        "$$H←  ​​​​​0 ​+​ \\frac{​​30(G−B)​​}{Vmax−Vmin}  ​​, if V​max​​ = R$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL0IP-MhjB7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download to /content/data/test.conll\n",
        "!wget https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/train.conll -P ./data\n",
        "!wget https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/dev.conll -P ./data\n",
        "!wget https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/test.conll -P ./data\n",
        "!wget https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/en-cw.txt -P ./data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDRNgoGlWqvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82192e8d-2d09-43bc-84f1-2df5bed4ee0d"
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6_I7HRfdpAc",
        "colab_type": "text"
      },
      "source": [
        "# parser_transitions.py [pyTorch]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xvgNWF3fhX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwr3mYCVfTQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PartialParse(object):\n",
        "    def __init__(self, sentence):\n",
        "        \"\"\"Initializes this partial parse.\n",
        "\n",
        "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
        "                                        Your code should not modify the sentence.\n",
        "        \"\"\"\n",
        "        # The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.\n",
        "        self.sentence = sentence\n",
        "\n",
        "        ### YOUR CODE HERE (3 Lines)\n",
        "        ### Your code should initialize the following fields:\n",
        "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
        "        ###                 last element of the list.\n",
        "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
        "        ###                  buffer as the first item of the list\n",
        "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
        "        ###             tuples where each tuple is of the form (head, dependent).\n",
        "        ###             Order for this list doesn't matter.\n",
        "        ###\n",
        "        ### Note: The root token should be represented with the string \"ROOT\"\n",
        "        ###\n",
        "        self.stack = [\"ROOT\"]\n",
        "        self.buffer = sentence.copy()\n",
        "        self.dependencies = []\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "\n",
        "    def parse_step(self, transition):\n",
        "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
        "\n",
        "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
        "                                left-arc, and right-arc transitions. You can assume the provided\n",
        "                                transition is a legal transition.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE (~7-10 Lines)\n",
        "        ### TODO:\n",
        "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
        "        ###     described in the pdf handout:\n",
        "        ###         1. Shift\n",
        "        ###         2. Left Arc\n",
        "        ###         3. Right Arc\n",
        "        if transition == \"S\":\n",
        "            self.stack.append(self.buffer[0])\n",
        "            del self.buffer[0]\n",
        "        if transition == \"LA\":\n",
        "            self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
        "            del self.stack[-2]\n",
        "        if transition == \"RA\":\n",
        "            self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
        "            del self.stack[-1]\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def parse(self, transitions):\n",
        "        \"\"\"Applies the provided transitions to this PartialParse\n",
        "\n",
        "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
        "\n",
        "        @return dsependencies (list of string tuples): The list of dependencies produced when\n",
        "                                                        parsing the sentence. Represented as a list of\n",
        "                                                        tuples where each tuple is of the form (head, dependent).\n",
        "        \"\"\"\n",
        "        for transition in transitions:\n",
        "            self.parse_step(transition)\n",
        "        return self.dependencies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5zhxxHcfs3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DummyModel(object):\n",
        "    \"\"\"Dummy model for testing the minibatch_parse function\n",
        "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
        "    the sentence is \"right\", \"left\" if otherwise.\n",
        "    \"\"\"\n",
        "    def predict(self, partial_parses):\n",
        "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1miJHndf1Z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_step(name, transition, stack, buf, deps,\n",
        "              ex_stack, ex_buf, ex_deps):\n",
        "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
        "    pp = PartialParse([])\n",
        "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
        "\n",
        "    pp.parse_step(transition)\n",
        "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
        "    assert stack == ex_stack, \\\n",
        "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
        "    assert buf == ex_buf, \\\n",
        "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "    print(\"{:} test passed!\".format(name))\n",
        "\n",
        "\n",
        "def test_parse_step():\n",
        "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
        "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
        "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
        "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
        "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
        "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAHvia65f61j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_parse():\n",
        "    \"\"\"Simple tests for the PartialParse.parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
        "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
        "    dependencies = tuple(sorted(dependencies))\n",
        "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
        "    assert dependencies == expected,  \\\n",
        "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
        "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
        "        \"parse test failed: the input sentence should not be modified\"\n",
        "    print(\"parse test passed!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ItfCRnccqkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
        "\n",
        "    @param sentences (list of list of str): A list of sentences to be parsed\n",
        "                                            (each sentence is a list of words and each word is of type string)\n",
        "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
        "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
        "                                returns a list of transitions predicted for each parse. That is, after calling\n",
        "                                    transitions = model.predict(partial_parses)\n",
        "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
        "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
        "\n",
        "\n",
        "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
        "                                                    list for a parsed sentence. Ordering should be the\n",
        "                                                    same as in sentences (i.e., dependencies[i] should\n",
        "                                                    contain the parse for sentences[i]).\n",
        "    \"\"\"\n",
        "    dependencies = []\n",
        "\n",
        "    ### YOUR CODE HERE (~8-10 Lines)\n",
        "    ### TODO:\n",
        "    ###     Implement the minibatch parse algorithm as described in the pdf handout\n",
        "    ###\n",
        "    ###     Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g.\n",
        "    ###                 unfinished_parses = partial_parses[:].\n",
        "    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
        "    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n",
        "    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n",
        "    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n",
        "    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n",
        "    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n",
        "    ###             is being accessed by `partial_parses` and may cause your code to crash.\n",
        "    \n",
        "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
        "    unfinished_parses = partial_parses[:]\n",
        "    while unfinished_parses:\n",
        "        minibatch = unfinished_parses[:batch_size]\n",
        "        transitions = model.predict(minibatch)\n",
        "        for pp, transition in zip(minibatch, transitions):\n",
        "            pp.parse_step(transition)\n",
        "            if len(pp.buffer) == 0 and len(pp.stack) == 1:\n",
        "                unfinished_parses.remove(pp)\n",
        "\n",
        "    dependencies = [pp.dependencies for pp in partial_parses]\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return dependencies\n",
        "\n",
        "\n",
        "def test_dependencies(name, deps, ex_deps):\n",
        "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
        "    deps = tuple(sorted(deps))\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "\n",
        "\n",
        "def test_minibatch_parse():\n",
        "    \"\"\"Simple tests for the minibatch_parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
        "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
        "                 [\"left\", \"arcs\", \"only\"],\n",
        "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[1],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[2],\n",
        "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[3],\n",
        "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
        "    print(\"minibatch_parse test passed!\")\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     args = sys.argv\n",
        "#     if len(args) != 2:\n",
        "#         raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n",
        "#     elif args[1] == \"part_c\":\n",
        "#         test_parse_step()\n",
        "#         test_parse()\n",
        "#     elif args[1] == \"part_d\":\n",
        "#         test_minibatch_parse()\n",
        "#     else:\n",
        "#         raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCMqBsU5d-V0",
        "colab_type": "text"
      },
      "source": [
        "## part_c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-QXnOFTdaCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c0a626d0-bca6-4e68-8520-5ea3aeee0f59"
      },
      "source": [
        "test_parse_step()\n",
        "test_parse()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHIFT test passed!\n",
            "LEFT-ARC test passed!\n",
            "RIGHT-ARC test passed!\n",
            "parse test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LExVBaT0eP3D",
        "colab_type": "text"
      },
      "source": [
        "## part_d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8crBA1PMeU24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0a04d64-1533-42d7-95f9-8bbd59c79f36"
      },
      "source": [
        "test_minibatch_parse()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minibatch_parse test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWGPsXLlg7xv",
        "colab_type": "text"
      },
      "source": [
        "# parser_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbaOtd2ThAli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ParserModel(nn.Module):\n",
        "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
        "    The ParserModel will predict which transition should be applied to a\n",
        "    given partial parse configuration.\n",
        "\n",
        "    PyTorch Notes:\n",
        "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
        "            are a subclass of this \"nn.Module\".\n",
        "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
        "            (embedding layers, linear layers, dropout layers, etc.).\n",
        "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
        "            when you write \"m = ParserModel()\".\n",
        "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
        "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
        "            in other ParserModel methods.\n",
        "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, n_features=36,\n",
        "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
        "        \"\"\" Initialize the parser model.\n",
        "\n",
        "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
        "        @param n_features (int): number of input features\n",
        "        @param hidden_size (int): number of hidden units\n",
        "        @param n_classes (int): number of output classes\n",
        "        @param dropout_prob (float): dropout probability\n",
        "        \"\"\"\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        ### YOUR CODE HERE (~5 Lines)\n",
        "        ### TODO:\n",
        "        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
        "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
        "        ###     2) Construct `self.dropout` layer.\n",
        "        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
        "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
        "        ###\n",
        "        ### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
        "        ###         It has been shown empirically, that this provides better initial weights\n",
        "        ###         for training networks than random uniform initialization.\n",
        "        ###         For more details checkout this great blogpost:\n",
        "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization \n",
        "        ### Hints:\n",
        "        ###     - After you create a linear layer you can access the weight\n",
        "        ###       matrix via:\n",
        "        ###         linear_layer.weight\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
        "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
        "\n",
        "        self.embed_to_hidden = nn.Linear(in_features=n_features*self.embed_size, out_features=hidden_size)\n",
        "        nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain=1)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.hidden_to_logits = nn.Linear(in_features=hidden_size, out_features=n_classes)\n",
        "        nn.init.xavier_uniform_(self.hidden_to_logits.weight, gain=1)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
        "            to embedding vectors.\n",
        "\n",
        "            PyTorch Notes:\n",
        "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
        "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
        "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
        "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
        "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
        "\n",
        "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
        "\n",
        "            @return x (Tensor): tensor of embeddings for words represented in t\n",
        "                                (batch_size, n_features * embed_size)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE (~1-3 Lines)\n",
        "        ### TODO:\n",
        "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
        "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
        "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
        "        ###\n",
        "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
        "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
        "        ###\n",
        "        ###  Please see the following docs for support:\n",
        "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
        "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
        "\n",
        "        x = self.pretrained_embeddings(t).view(t.size(0), self.n_features * self.embed_size)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, t):\n",
        "        \"\"\" Run the model forward.\n",
        "\n",
        "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
        "\n",
        "            PyTorch Notes:\n",
        "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
        "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
        "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
        "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
        "                        model = ParserModel()\n",
        "                        output = model(t) # this calls the forward function\n",
        "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
        "\n",
        "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
        "\n",
        "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
        "                                 without applying softmax (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "        ###  YOUR CODE HERE (~3-5 lines)\n",
        "        ### TODO:\n",
        "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
        "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
        "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
        "        ###     4) Apply dropout layer to the output of step 3.\n",
        "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
        "        ###\n",
        "        ### Note: We do not apply the softmax to the logits here, because\n",
        "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
        "        \n",
        "        embeddings = self.embedding_lookup(t)\n",
        "        hidden_state = self.embed_to_hidden(embeddings)\n",
        "        hidden_relu = torch.relu(hidden_state)\n",
        "        hidden_dropout = self.dropout(hidden_relu)\n",
        "        logits = self.hidden_to_logits(hidden_dropout)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "        return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snNSrM81nHZT",
        "colab_type": "text"
      },
      "source": [
        "# general_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCcJWeJinQ7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "general_utils.py: General purpose utilities.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
        "    iterate through data in minibatches as follows:\n",
        "\n",
        "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
        "            ...\n",
        "\n",
        "    Or with multiple data sources:\n",
        "\n",
        "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
        "            ...\n",
        "\n",
        "    Args:\n",
        "        data: there are two possible values:\n",
        "            - a list or numpy array\n",
        "            - a list where each element is either a list or numpy array\n",
        "        minibatch_size: the maximum number of items in a minibatch\n",
        "        shuffle: whether to randomize the order of returned data\n",
        "    Returns:\n",
        "        minibatches: the return value depends on data:\n",
        "            - If data is a list/array it yields the next minibatch of data.\n",
        "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
        "              list. This can be used to iterate through multiple data sources\n",
        "              (e.g., features and labels) at the same time.\n",
        "\n",
        "    \"\"\"\n",
        "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
        "    data_size = len(data[0]) if list_data else len(data)\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
        "            else _minibatch(data, minibatch_indices)\n",
        "\n",
        "\n",
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
        "\n",
        "\n",
        "def test_all_close(name, actual, expected):\n",
        "    if actual.shape != expected.shape:\n",
        "        raise ValueError(\"{:} failed, expected output to have shape {:} but has shape {:}\"\n",
        "                         .format(name, expected.shape, actual.shape))\n",
        "    if np.amax(np.fabs(actual - expected)) > 1e-6:\n",
        "        raise ValueError(\"{:} failed, expected {:} but value is {:}\".format(name, expected, actual))\n",
        "    else:\n",
        "        print(name, \"passed!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iwiumCghl7A",
        "colab_type": "text"
      },
      "source": [
        "# parser_util.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyT0U29biMkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "parser_utils.py: Utilities for training the dependency parser.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "from collections import Counter\n",
        "# from . general_utils import get_minibatches\n",
        "# from parser_transitions import minibatch_parse\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "P_PREFIX = '<p>:'\n",
        "L_PREFIX = '<l>:'\n",
        "UNK = '<UNK>'\n",
        "NULL = '<NULL>'\n",
        "ROOT = '<ROOT>'\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    language = 'english'\n",
        "    with_punct = True\n",
        "    unlabeled = True\n",
        "    lowercase = True\n",
        "    use_pos = True\n",
        "    use_dep = True\n",
        "    use_dep = use_dep and (not unlabeled)\n",
        "    data_path = './data'\n",
        "    train_file = 'train.conll'\n",
        "    dev_file = 'dev.conll'\n",
        "    test_file = 'test.conll'\n",
        "    embedding_file = './data/en-cw.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJNvv69chkoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Parser(object):\n",
        "    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        root_labels = list([l for ex in dataset\n",
        "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
        "        counter = Counter(root_labels)\n",
        "        if len(counter) > 1: # <- A data sanity check that all labels corresponding to head = 0 are called 'root' (and not something else like 'det' 'amod' 'nsubj' 'punct' etc.)\n",
        "            logging.info('Warning: more than one root label')\n",
        "            logging.info(counter)\n",
        "        self.root_label = counter.most_common()[0][0]\n",
        "        # deprel <- list of all unique labels\n",
        "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
        "                                               for w in ex['label']\n",
        "                                               if w != self.root_label]))\n",
        "        # tok2id <- dictionary of {label -> (arbitrary) id}\n",
        "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
        "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
        "\n",
        "        config = Config()\n",
        "        self.unlabeled = config.unlabeled\n",
        "        self.with_punct = config.with_punct\n",
        "        self.use_pos = config.use_pos\n",
        "        self.use_dep = config.use_dep\n",
        "        self.language = config.language\n",
        "        \n",
        "        # create a list of possible transitions based on if transitions are labled or unlabeled\n",
        "        if self.unlabeled:\n",
        "            trans = ['L', 'R', 'S']\n",
        "            self.n_deprel = 1\n",
        "        else:\n",
        "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
        "            self.n_deprel = len(deprel)\n",
        "\n",
        "        self.n_trans = len(trans)\n",
        "        # Create two dictionaries mapping {transition -> id} and {id -> transition}\n",
        "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
        "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
        "\n",
        "        # logging.info('Build dictionary for part-of-speech tags.')\n",
        "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
        "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
        "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
        "        \n",
        "        # logging.info('Build dictionary for words.')\n",
        "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[UNK] = self.UNK = len(tok2id)\n",
        "        tok2id[NULL] = self.NULL = len(tok2id)\n",
        "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
        "\n",
        "        self.tok2id = tok2id  # <- create reverse dict {id->token}\n",
        "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
        "\n",
        "        # Number of input features = 18 + 18 if using POS + 12 if using dep_labels\n",
        "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
        "        self.n_tokens = len(tok2id)\n",
        "\n",
        "    def vectorize(self, examples):\n",
        "        vec_examples = []\n",
        "        for ex in examples:\n",
        "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
        "                                  else self.UNK for w in ex['word']]\n",
        "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
        "                                   else self.P_UNK for w in ex['pos']]\n",
        "            head = [-1] + ex['head']\n",
        "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
        "                            else -1 for w in ex['label']]\n",
        "            vec_examples.append({'word': word, 'pos': pos,\n",
        "                                 'head': head, 'label': label})\n",
        "        return vec_examples\n",
        "\n",
        "    def extract_features(self, stack, buf, arcs, ex):\n",
        "        # print(\"extract_features -------\")\n",
        "        # print(\"stack\", stack)\n",
        "        # print(\"buffer\", buf)\n",
        "        # print(\"arcs\", arcs)\n",
        "        # print(\"ex\", ex)\n",
        "        if stack[0] == \"ROOT\":\n",
        "            stack[0] = 0\n",
        "\n",
        "        def get_lc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
        "\n",
        "        def get_rc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
        "                          reverse=True)\n",
        "\n",
        "        p_features = []\n",
        "        l_features = []\n",
        "        # print(self.NULL, (3 - len(stack)), [ex['word'][x] for x in stack[-3:]])\n",
        "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
        "        # print(features)\n",
        "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
        "        # print(features)\n",
        "        if self.use_pos:\n",
        "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
        "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
        "            # print(\"self.P_NULL\", self.P_NULL, self.id2tok[83])\n",
        "            # print(\"p_feat\", p_features)\n",
        "\n",
        "        for i in range(2):\n",
        "            # print(\"i\", i, \"len(stack)\", len(stack))\n",
        "            if i < len(stack):\n",
        "                k = stack[-i-1]\n",
        "                # print('k', k)\n",
        "                lc = get_lc(k)\n",
        "                # print(\"lc\", lc)\n",
        "                rc = get_rc(k)\n",
        "                # print(\"rc\", rc)\n",
        "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
        "                # print(\"llc\", llc)\n",
        "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
        "                # print(\"rrc\", rrc)\n",
        "\n",
        "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
        "\n",
        "                if self.use_pos:\n",
        "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
        "                    # print(\"p_feat:\", p_features)\n",
        "\n",
        "                if self.use_dep:\n",
        "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
        "                # print(features)\n",
        "            else:\n",
        "                features += [self.NULL] * 6\n",
        "                # print(features)\n",
        "                if self.use_pos:\n",
        "                    p_features += [self.P_NULL] * 6\n",
        "                if self.use_dep:\n",
        "                    l_features += [self.L_NULL] * 6\n",
        "\n",
        "        # print(\"p_features\", p_features, \"l_features\", l_features)\n",
        "        features += p_features + l_features\n",
        "        # print(\"features\", features, \"self.n_features\", self.n_features)\n",
        "        assert len(features) == self.n_features\n",
        "        return features\n",
        "\n",
        "    def get_oracle(self, stack, buf, ex):\n",
        "        # print(\"get_oracle --------\")\n",
        "        # print(\"self.n_trans: \", self.n_trans, \", len(stack)\", len(stack))\n",
        "        if len(stack) < 2:\n",
        "            return self.n_trans - 1\n",
        "\n",
        "        i0 = stack[-1]\n",
        "        i1 = stack[-2]\n",
        "        h0 = ex['head'][i0]\n",
        "        h1 = ex['head'][i1]\n",
        "        l0 = ex['label'][i0]\n",
        "        l1 = ex['label'][i1]\n",
        "\n",
        "        if self.unlabeled:\n",
        "            if (i1 > 0) and (h1 == i0):\n",
        "                return 0\n",
        "            elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "                return 1\n",
        "            else:\n",
        "                return None if len(buf) == 0 else 2\n",
        "        else:\n",
        "            if (i1 > 0) and (h1 == i0):\n",
        "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
        "            elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
        "            else:\n",
        "                return None if len(buf) == 0 else self.n_trans - 1\n",
        "\n",
        "    def create_instances(self, examples):\n",
        "        all_instances = []\n",
        "        succ = 0\n",
        "        for id, ex in enumerate(examples):\n",
        "            # print(id, ex)\n",
        "            n_words = len(ex['word']) - 1\n",
        "\n",
        "            # arcs = {(h, t, label)}\n",
        "            stack = [0]\n",
        "            buf = [i + 1 for i in range(n_words)]\n",
        "            arcs = []\n",
        "            instances = []\n",
        "            for i in range(n_words * 2):\n",
        "                # print(i, \"---------------------------------------------------------------\")\n",
        "                # print(\"stack\", stack)\n",
        "                # print(\"buffer\", buf)\n",
        "                # print(\"arcs\", arcs)\n",
        "                # print(\"instances\", instances)\n",
        "                gold_t = self.get_oracle(stack, buf, ex)\n",
        "                # print(\"gold_t\", gold_t)\n",
        "                # sys.exit()\n",
        "                if gold_t is None:\n",
        "                    break\n",
        "                legal_labels = self.legal_labels(stack, buf)\n",
        "                # print(\"legal_labels:\", legal_labels)\n",
        "                assert legal_labels[gold_t] == 1\n",
        "                # sys.exit()\n",
        "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
        "                                  legal_labels, gold_t))\n",
        "                # print(\"instances\", instances)\n",
        "                if gold_t == self.n_trans - 1:\n",
        "                    # Shift\n",
        "                    stack.append(buf[0])\n",
        "                    buf = buf[1:]\n",
        "                elif gold_t < self.n_deprel:\n",
        "                    # Left Arc\n",
        "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
        "                    stack = stack[:-2] + [stack[-1]]\n",
        "                else:\n",
        "                    # Right Arc\n",
        "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
        "                    stack = stack[:-1]\n",
        "            else:\n",
        "                succ += 1\n",
        "                all_instances += instances\n",
        "            # print(\"INSTANCES\", instances)\n",
        "            # print(\"ID\", id)\n",
        "            # sys.exit()\n",
        "        return all_instances\n",
        "\n",
        "    def legal_labels(self, stack, buf):\n",
        "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
        "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
        "        labels += [1] if len(buf) > 0 else [0]\n",
        "        return labels\n",
        "\n",
        "    def parse(self, dataset, eval_batch_size=5000):\n",
        "        sentences = []\n",
        "        sentence_id_to_idx = {}\n",
        "        # print(\" Parse.parse() =======\")\n",
        "        # print(len(dataset))\n",
        "        # print(dataset[0])\n",
        "        for i, example in enumerate(dataset):\n",
        "            n_words = len(example['word']) - 1\n",
        "            sentence = [j + 1 for j in range(n_words)]\n",
        "            sentences.append(sentence)\n",
        "            sentence_id_to_idx[id(sentence)] = i\n",
        "\n",
        "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
        "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
        "\n",
        "        UAS = all_tokens = 0.0\n",
        "        with tqdm(total=len(dataset)) as prog:\n",
        "            for i, ex in enumerate(dataset):\n",
        "                head = [-1] * len(ex['word'])\n",
        "                for h, t, in dependencies[i]:\n",
        "                    head[t] = h\n",
        "                for pred_h, gold_h, gold_l, pos in \\\n",
        "                        zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
        "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                        if (self.with_punct) or (not punct(self.language, pos_str)):\n",
        "                            UAS += 1 if pred_h == gold_h else 0\n",
        "                            all_tokens += 1\n",
        "                prog.update(i + 1)\n",
        "        UAS /= all_tokens\n",
        "        return UAS, dependencies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlfOtRNSiGuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelWrapper(object):\n",
        "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
        "        self.parser = parser\n",
        "        self.dataset = dataset\n",
        "        self.sentence_id_to_idx = sentence_id_to_idx\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
        "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
        "                for p in partial_parses]\n",
        "        mb_x = np.array(mb_x).astype('int32')\n",
        "        # mb_x = torch.from_numpy(mb_x).long()\n",
        "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
        "\n",
        "        # pred = self.parser.model(mb_x)  ## [TODO: model.evaluate or model.predict might be better]\n",
        "        pred = self.parser.model(mb_x, training=False)  ## [TODO: model.evaluate or model.predict might be better]\n",
        "        # pred = pred.detach().numpy()\n",
        "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
        "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
        "        return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjzFlcN_h-LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_conll(in_file, lowercase=False, max_example=None):\n",
        "    examples = []\n",
        "    with open(in_file) as f:\n",
        "        word, pos, head, label = [], [], [], []\n",
        "        for line in f.readlines():\n",
        "            sp = line.strip().split('\\t')\n",
        "            if len(sp) == 10:\n",
        "                if '-' not in sp[0]: # <- Ignore words which have a '-'\n",
        "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
        "                    pos.append(sp[4])\n",
        "                    head.append(int(sp[6]))\n",
        "                    label.append(sp[7])\n",
        "            elif len(word) > 0: # <- We have reached an empty row, indicating end of a sentence\n",
        "                examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
        "                word, pos, head, label = [], [], [], []\n",
        "                if (max_example is not None) and (len(examples) == max_example):\n",
        "                    break\n",
        "        if len(word) > 0: # <- Incase the very last line is not an empty row, add the last sentence too\n",
        "            examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
        "    return examples\n",
        "\n",
        "\n",
        "def build_dict(keys, n_max=None, offset=0):\n",
        "    count = Counter()\n",
        "    for key in keys:\n",
        "        count[key] += 1\n",
        "    ls = count.most_common() if n_max is None \\\n",
        "        else count.most_common(n_max)\n",
        "\n",
        "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n",
        "\n",
        "\n",
        "def punct(language, pos):\n",
        "    if language == 'english':\n",
        "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
        "    elif language == 'chinese':\n",
        "        return pos == 'PU'\n",
        "    elif language == 'french':\n",
        "        return pos == 'PUNC'\n",
        "    elif language == 'german':\n",
        "        return pos in [\"$.\", \"$,\", \"$[\"]\n",
        "    elif language == 'spanish':\n",
        "        # http://nlp.stanford.edu/software/spanish-faq.shtml\n",
        "        return pos in [\"f0\", \"faa\", \"fat\", \"fc\", \"fd\", \"fe\", \"fg\", \"fh\",\n",
        "                       \"fia\", \"fit\", \"fp\", \"fpa\", \"fpt\", \"fs\", \"ft\",\n",
        "                       \"fx\", \"fz\"]\n",
        "    elif language == 'universal':\n",
        "        return pos == 'PUNCT'\n",
        "    else:\n",
        "        raise ValueError('language: %s is not supported.' % language)\n",
        "\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1gJEugeh4SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_preprocess_data(reduced=True):\n",
        "    config = Config()\n",
        "\n",
        "    print(\"Loading data...\",)\n",
        "    start = time.time()\n",
        "    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n",
        "                           lowercase=config.lowercase)\n",
        "    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n",
        "                         lowercase=config.lowercase)\n",
        "    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n",
        "                          lowercase=config.lowercase)\n",
        "    if reduced:\n",
        "        train_set = train_set[:1000]\n",
        "        dev_set = dev_set[:500]\n",
        "        test_set = test_set[:500]\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Building parser...\",)\n",
        "    start = time.time()\n",
        "    parser = Parser(train_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "    \n",
        "    print(\"Loading pretrained embeddings...\",)\n",
        "    start = time.time()\n",
        "    word_vectors = {}\n",
        "    # [TODO] Show/describe what does embedding_file looks like\n",
        "    for line in open(config.embedding_file).readlines():\n",
        "        sp = line.strip().split()\n",
        "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
        "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
        "\n",
        "    for token in parser.tok2id:\n",
        "        i = parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token]\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "    # [TODO] Show what does embeddings_matrix looks like, its shape\n",
        "\n",
        "    print(\"Vectorizing data...\",)\n",
        "    start = time.time()\n",
        "    train_set = parser.vectorize(train_set)\n",
        "    dev_set = parser.vectorize(dev_set)\n",
        "    test_set = parser.vectorize(test_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Preprocessing training data...\",)\n",
        "    start = time.time()\n",
        "    train_examples = parser.create_instances(train_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "    # print(\"=\"*15)\n",
        "    # print(train_examples[:3])\n",
        "\n",
        "    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JejZyKWuhyJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm6Qv4znhJfe",
        "colab_type": "text"
      },
      "source": [
        "# run.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsqMozdHhFC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69aa276e-e369-4ce7-ca82-f835875cc30a"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "run.py: Run the dependency parser.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from parser_model import ParserModel\n",
        "# from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n",
        "\n",
        "# -----------------\n",
        "# Primary Functions\n",
        "# -----------------\n",
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \"\"\" Train the neural dependency parser.\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param output_path (str): Path to which model weights and results are written.\n",
        "    @param batch_size (int): Number of examples in a single batch\n",
        "    @param n_epochs (int): Number of training epochs\n",
        "    @param lr (float): Learning rate\n",
        "    \"\"\"\n",
        "    best_dev_UAS = 0\n",
        "\n",
        "\n",
        "    ### YOUR CODE HERE (~2-7 lines)\n",
        "    ### TODO:\n",
        "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
        "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func`\n",
        "    ###\n",
        "    ### Hint: Use `parser.model.parameters()` to pass optimizer\n",
        "    ###       necessary parameters to tune.\n",
        "    ### Please see the following docs for support:\n",
        "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
        "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "\n",
        "    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \"\"\" Train the neural dependency parser for single epoch.\n",
        "\n",
        "    Note: In PyTorch we can signify train versus test and automatically have\n",
        "    the Dropout Layer applied and removed, accordingly, by specifying\n",
        "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
        "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
        "    @param batch_size (int): batch size\n",
        "    @param lr (float): learning rate\n",
        "\n",
        "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
        "    \"\"\"\n",
        "    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            optimizer.zero_grad()   # remove any baggage in the optimizer\n",
        "            loss = 0. # store loss for this batch here\n",
        "            train_x = torch.from_numpy(train_x).long()\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
        "\n",
        "            ### YOUR CODE HERE (~5-10 lines)\n",
        "            ### TODO:\n",
        "            ###      1) Run train_x forward through model to produce `logits`\n",
        "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
        "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
        "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
        "            ###         are the predictions (y^ from the PDF).\n",
        "            ###      3) Backprop losses\n",
        "            ###      4) Take step with the optimizer\n",
        "            ### Please see the following docs for support:\n",
        "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
        "\n",
        "            logits = parser.model(train_x)\n",
        "            loss = loss_func(logits, train_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            ### END YOUR CODE\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Note: Set debug to False, when training on entire corpus\n",
        "    debug = True\n",
        "    # debug = False\n",
        "    print(torch.__version__)\n",
        "    # assert(torch.__version__ == \"1.0.0\"),  \"Please install torch version 1.0.0\"\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"INITIALIZING\")\n",
        "    print(80 * \"=\")\n",
        "    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n",
        "\n",
        "    start = time.time()\n",
        "    model = ParserModel(embeddings)\n",
        "    \n",
        "    use_cuda = True\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "    parser.model = model\n",
        "\n",
        "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"TRAINING\")\n",
        "    print(80 * \"=\")\n",
        "    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "    output_path = output_dir + \"model.weights\"\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
        "\n",
        "    if not debug:\n",
        "        print(80 * \"=\")\n",
        "        print(\"TESTING\")\n",
        "        print(80 * \"=\")\n",
        "        print(\"Restoring the best model weights found on the dev set\")\n",
        "        parser.model.load_state_dict(torch.load(output_path))\n",
        "        print(\"Final evaluation on test set\",)\n",
        "        parser.model.eval()\n",
        "        UAS, dependencies = parser.parse(test_data)\n",
        "        print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "        print(\"Done!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "================================================================================\n",
            "INITIALIZING\n",
            "================================================================================\n",
            "Loading data...\n",
            "took 2.15 seconds\n",
            "Building parser...\n",
            "took 0.03 seconds\n",
            "Loading pretrained embeddings...\n",
            "took 2.28 seconds\n",
            "Vectorizing data...\n",
            "took 0.06 seconds\n",
            "Preprocessing training data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "took 1.41 seconds\n",
            "took 0.14 seconds\n",
            "\n",
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:05<00:00,  9.55it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.6449052927394708\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11511955.47it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 51.75\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:04<00:00,  9.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.3506443326671918\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 12360860.61it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 60.10\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:04<00:00,  9.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.2803200262909134\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 10776355.95it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 62.88\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 4 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:04<00:00,  9.87it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.24698927470793328\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 10919261.21it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 66.10\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 5 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:05<00:00,  9.56it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.2166077879567941\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 9814238.83it/s]       \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 66.82\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 6 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:04<00:00,  9.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.19686038667956987\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 12010987.61it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 67.95\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 7 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:04<00:00,  9.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.18050935678184032\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 10556558.48it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 69.77\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 8 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:04<00:00,  9.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.16459698447336754\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 12042651.26it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 71.08\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 9 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:05<00:00,  9.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.15182308790584406\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11495078.36it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 71.31\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 10 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:04<00:00,  9.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.14128982198114196\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11366001.21it/s]      "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 71.95\n",
            "New best dev UAS! Saving model.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQQSjy9jqwz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq0fl-lf8Txx",
        "colab_type": "text"
      },
      "source": [
        "# parser_model.py [TF]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chy4DRvX8cBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ParserModel(nn.Module):\n",
        "    def __init__(self, embeddings, n_features=36,\n",
        "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "---->\n",
        "        self.pretrained_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=300, \n",
        "                                      input_length=Length_of_input_sequences, \n",
        "                                      embeddings_initializer=matrix_of_pretrained_weights)\n",
        "        \n",
        "        self.pretrained_embeddings = Embedding(vocab_size, embedding_dim,\n",
        "                              input_length=1, \n",
        "                              name='embedding')\n",
        "        self.pretrained_embeddings.build(input_shape=(1,)) # the input_shape here has no effect in the build function\n",
        "        self.pretrained_embeddings.set_weights([pretrained_embeddings])\n",
        "<----\n",
        "\n",
        "        ### YOUR CODE HERE (~5 Lines)\n",
        "\n",
        "        self.embed_to_hidden = nn.Linear(in_features=n_features*self.embed_size, out_features=hidden_size)\n",
        "        nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain=1)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.hidden_to_logits = nn.Linear(in_features=hidden_size, out_features=n_classes)\n",
        "        nn.init.xavier_uniform_(self.hidden_to_logits.weight, gain=1)\n",
        "---->\n",
        "        # Glorot normal:\n",
        "        # https://stackoverflow.com/questions/43284047/what-is-the-default-kernel-initializer-in-tf-layers-conv2d-and-tf-layers-dense\n",
        "        init = tf.initializers.GlorotUniform()\n",
        "        self.embed_to_hidden = Dense(in_features=n_features*self.embed_size, out_features=hidden_size, \n",
        "                                     kernel_initializer='glorot_uniform')\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=.2, input_shape=(2,))\n",
        "        self.hidden_to_logits = Dense(in_features=hidden_size, out_features=n_classes, \n",
        "                                     kernel_initializer='glorot_uniform')\n",
        "<----\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        ### YOUR CODE HERE (~1-3 Lines)\n",
        "        \n",
        "        x = self.pretrained_embeddings(t).view(t.size(0), self.n_features * self.embed_size)\n",
        "---->\n",
        "        x = self.pretrained_embeddings(t) <--- view???\n",
        "<----\n",
        "        ### END YOUR CODE\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, t):\n",
        "\n",
        "        ###  YOUR CODE HERE (~3-5 lines)\n",
        "\n",
        "        embeddings = self.embedding_lookup(t)\n",
        "        hidden_state = self.embed_to_hidden(embeddings)\n",
        "        hidden_relu = torch.relu(hidden_state)\n",
        "        hidden_dropout = self.dropout(hidden_relu)\n",
        "        logits = self.hidden_to_logits(hidden_dropout)\n",
        "---->\n",
        "embeddings = self.embedding_lookup(t)\n",
        "hidden_state = self.embed_to_hidden(embeddings)\n",
        "hidden_relu = tf.nn.relu(hidden_state)\n",
        "hidden_dropout = self.dropout(hidden_relu)\n",
        "logits = self.hidden_to_logits(hidden_dropout)\n",
        "<----\n",
        "        ### END YOUR CODE\n",
        "        return logits\n",
        "\n",
        "\n",
        "resnet = ParserModel()\n",
        "dataset = ...\n",
        "resnet.fit(dataset, epochs=10)\n",
        "resnet.save_weights(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHYbjBYAHaYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# import tensorflow.keras as keras\n",
        "class ParserModel(tf.keras.Model):\n",
        "    def __init__(self, embeddings, n_features=36, hidden_size=200, n_classes=3, \n",
        "                 dropout_prob=0.5):\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        # self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        # ---->\n",
        "        # self.pretrained_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=300, \n",
        "        #                               input_length=Length_of_input_sequences, \n",
        "        #                               embeddings_initializer=matrix_of_pretrained_weights)\n",
        "        \n",
        "        ## Note: Embedding layer is essentially a lookup table\n",
        "        # embedding shape = (num_of_tokens, embed_size)\n",
        "        # Shape of the expected input = (batch, n_features)\n",
        "        # output shape = (batch, n_features, embed_size) - OR - (batch, n_features * emebd_size)  \n",
        "        self.pretrained_embeddings = tf.keras.layers.Embedding(input_dim=embeddings.shape[0], output_dim=self.embed_size,\n",
        "                              input_length=n_features, name='embedding')\n",
        "        self.pretrained_embeddings.build(input_shape=(1,)) # the input_shape here has no effect in the build function\n",
        "        # self.pretrained_embeddings.build() # the input_shape here has no effect in the build function\n",
        "        self.pretrained_embeddings.set_weights([embeddings])\n",
        "        # print(\" == embeddings == \")\n",
        "        # print(embeddings.shape)\n",
        "        # print(embeddings)\n",
        "        # print(embeddings[85])\n",
        "        # print(embeddings[86])\n",
        "        # print(embeddings[87])\n",
        "        # print(embeddings[88])\n",
        "        # print(self.pretrained_embeddings.get_weights())\n",
        "        # <----\n",
        "\n",
        "        ### YOUR CODE HERE (~5 Lines)\n",
        "\n",
        "        # self.embed_to_hidden = nn.Linear(in_features=n_features*self.embed_size, out_features=hidden_size)\n",
        "        # nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain=1)\n",
        "        # self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        # self.hidden_to_logits = nn.Linear(in_features=hidden_size, out_features=n_classes)\n",
        "        # nn.init.xavier_uniform_(self.hidden_to_logits.weight, gain=1)\n",
        "        \n",
        "        # ---->\n",
        "        # Glorot normal:\n",
        "        # https://stackoverflow.com/questions/43284047/what-is-the-default-kernel-initializer-in-tf-layers-conv2d-and-tf-layers-dense\n",
        "        # init = tf.initializers.GlorotUniform()\n",
        "        # tf.random.set_seed(0)\n",
        "        self.embed_to_hidden = tf.keras.layers.Dense(units=hidden_size, \n",
        "                                     kernel_initializer='glorot_uniform')\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=dropout_prob)\n",
        "        self.hidden_to_logits = tf.keras.layers.Dense(units=n_classes,\n",
        "                                     kernel_initializer='glorot_uniform')\n",
        "        # <----\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        ### YOUR CODE HERE (~1-3 Lines)\n",
        "        # x = self.pretrained_embeddings(t).view(t.size(0), self.n_features * self.embed_size)\n",
        "        # ---->\n",
        "        # print(\" == t == \")\n",
        "        # print(t.shape)\n",
        "        # print(t)\n",
        "        x = self.pretrained_embeddings(t)\n",
        "        x = tf.reshape(x, [t.shape[0], self.n_features*self.embed_size])\n",
        "        # print(\" == x == \")\n",
        "        # print(x.shape, x.shape[0])\n",
        "        # print(tf.size(x))\n",
        "        # print(x)\n",
        "        # print(\"-=-=-=\")\n",
        "        # print(t[0][0])\n",
        "        # print(self.pretrained_embeddings.get_weights()[0][t[0][0]])\n",
        "        # print(self.pretrained_embeddings.get_weights()[0][t[0][35]])\n",
        "        # print(self.pretrained_embeddings.get_weights()[0][t[0][36]])\n",
        "        # <----\n",
        "        ### END YOUR CODE\n",
        "        return x\n",
        "\n",
        "\n",
        "    def call(self, t):\n",
        "        ###  YOUR CODE HERE (~3-5 lines)\n",
        "        # embeddings = self.embedding_lookup(t)\n",
        "        # hidden_state = self.embed_to_hidden(embeddings)\n",
        "        # hidden_relu = torch.relu(hidden_state)\n",
        "        # hidden_dropout = self.dropout(hidden_relu)\n",
        "        # logits = self.hidden_to_logits(hidden_dropout)\n",
        "        # ---->\n",
        "        embeddings = self.embedding_lookup(t)\n",
        "        hidden_state = self.embed_to_hidden(embeddings)\n",
        "        hidden_relu = tf.nn.relu(hidden_state)\n",
        "        hidden_dropout = self.dropout(hidden_relu)\n",
        "        logits = self.hidden_to_logits(hidden_dropout)\n",
        "        # <----\n",
        "        ### END YOUR CODE\n",
        "        return logits\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe0N7RedVf1T",
        "colab_type": "text"
      },
      "source": [
        "# run.py [TF]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBei9SeVdbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ffe686c-5ed7-4bb6-ea5b-7b1631d82dfe"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "run.py: Run the dependency parser.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from parser_model import ParserModel\n",
        "# from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n",
        "\n",
        "# -----------------\n",
        "# Primary Functions\n",
        "# -----------------\n",
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \"\"\" Train the neural dependency parser.\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param output_path (str): Path to which model weights and results are written.\n",
        "    @param batch_size (int): Number of examples in a single batch\n",
        "    @param n_epochs (int): Number of training epochs\n",
        "    @param lr (float): Learning rate\n",
        "    \"\"\"\n",
        "    best_dev_UAS = 0\n",
        "\n",
        "\n",
        "    ### YOUR CODE HERE (~2-7 lines)\n",
        "    ### TODO:\n",
        "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
        "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func`\n",
        "    ###\n",
        "    ### Hint: Use `parser.model.parameters()` to pass optimizer\n",
        "    ###       necessary parameters to tune.\n",
        "    ### Please see the following docs for support:\n",
        "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
        "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "\n",
        "    # optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
        "    # loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-08, name='Adam')\n",
        "    # loss_func = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
        "    loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            # torch.save(parser.model.state_dict(), output_path)\n",
        "            parser.model.save_weights(output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \"\"\" Train the neural dependency parser for single epoch.\n",
        "\n",
        "    Note: In PyTorch we can signify train versus test and automatically have\n",
        "    the Dropout Layer applied and removed, accordingly, by specifying\n",
        "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
        "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
        "    @param batch_size (int): batch size\n",
        "    @param lr (float): learning rate\n",
        "\n",
        "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
        "    \"\"\"\n",
        "    # parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            # optimizer.zero_grad()   # remove any baggage in the optimizer\n",
        "            loss = 0. # store loss for this batch here\n",
        "            # train_x = torch.from_numpy(train_x).long()\n",
        "            # train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
        "\n",
        "            ### YOUR CODE HERE (~5-10 lines)\n",
        "            ### TODO:\n",
        "            ###      1) Run train_x forward through model to produce `logits`\n",
        "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
        "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
        "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
        "            ###         are the predictions (y^ from the PDF).\n",
        "            ###      3) Backprop losses\n",
        "            ###      4) Take step with the optimizer\n",
        "            ### Please see the following docs for support:\n",
        "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
        "\n",
        "            # logits = parser.model(train_x)\n",
        "            # loss = loss_func(logits, train_y)\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "              logits = parser.model(train_x, training=True) # <- can be above gradient tape too\n",
        "              loss = loss_func(train_y, logits)\n",
        "              # print(\"loss - \", loss)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            ### END YOUR CODE\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.numpy())\n",
        "\n",
        "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    # parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Note: Set debug to False, when training on entire corpus\n",
        "    debug = True\n",
        "    # debug = False\n",
        "    print(torch.__version__)\n",
        "    # assert(torch.__version__ == \"1.0.0\"),  \"Please install torch version 1.0.0\"\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"INITIALIZING\")\n",
        "    print(80 * \"=\")\n",
        "    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n",
        "\n",
        "    start = time.time()\n",
        "    model = ParserModel(embeddings)\n",
        "    \n",
        "    use_cuda = True\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "    parser.model = model\n",
        "\n",
        "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"TRAINING\")\n",
        "    print(80 * \"=\")\n",
        "    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "    output_path = output_dir + \"model.weights\"\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
        "\n",
        "    if not debug:\n",
        "        print(80 * \"=\")\n",
        "        print(\"TESTING\")\n",
        "        print(80 * \"=\")\n",
        "        print(\"Restoring the best model weights found on the dev set\")\n",
        "        # parser.model.load_state_dict(torch.load(output_path))\n",
        "        parser.model.load_weights(output_path)\n",
        "        print(\"Final evaluation on test set\",)\n",
        "        # parser.model.eval()\n",
        "        UAS, dependencies = parser.parse(test_data)\n",
        "        print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "        print(\"Done!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "================================================================================\n",
            "INITIALIZING\n",
            "================================================================================\n",
            "Loading data...\n",
            "took 2.25 seconds\n",
            "Building parser...\n",
            "took 0.06 seconds\n",
            "Loading pretrained embeddings...\n",
            "took 2.43 seconds\n",
            "Vectorizing data...\n",
            "took 0.07 seconds\n",
            "Preprocessing training data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "took 1.28 seconds\n",
            "took 0.01 seconds\n",
            "\n",
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 13.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.2731301225721836\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11498600.83it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 8.37\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.8466845937073231\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 9500100.84it/s]       \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 35.03\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 3.353625108798345\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11833770.55it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "\n",
            "Epoch 4 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.2357710699240367\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11045300.37it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "\n",
            "Epoch 5 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.0723077729344368\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 10580370.90it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "\n",
            "Epoch 6 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.0083942897617817\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11727571.74it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "\n",
            "Epoch 7 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.9479707889258862\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11859684.31it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "\n",
            "Epoch 8 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.884236595282952\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 10847561.92it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "\n",
            "Epoch 9 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.8106130460898081\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11564667.28it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "\n",
            "Epoch 10 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 14.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.7725006975233555\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 10516196.10it/s]      "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5UhSO-sawOU",
        "colab_type": "text"
      },
      "source": [
        "### Train Data\n",
        "So what does the training data looks like?\n",
        "\n",
        "Training data is a tab seprated text file with 10 columns.\n",
        "\n",
        "Following is the screenshot of a part of the train.conll showing two sentences:\n",
        "1.   The new rate will be payable Feb. 15.\n",
        "2.   A record date hasn't been set.\n",
        "\n",
        "![train.conll](https://github.com/AbhishekDutt/cs224n-tensorflow/raw/devlop/A3/screenshots/train.conll.1.annotated.png)\n",
        "\n",
        "TODO: Add details about what each column means\n",
        "\n",
        "Each of the train, dev and test data is converted into a list of dictionries by read_conll() function.\n",
        "```\n",
        "[\n",
        "  {\n",
        "    'word': ['the', 'new', 'rate', 'will', 'be', 'payable', 'feb.', '15', '.'], \n",
        "    'pos': ['DT', 'JJ', 'NN', 'MD', 'VB', 'JJ', 'NNP', 'CD', '.'], \n",
        "    'head': [3, 3, 6, 6, 6, 0, 6, 7, 6], \n",
        "    'label': ['det', 'amod', 'nsubj', 'aux', 'cop', 'root', 'nmod:tmod', 'nummod', 'punct']\n",
        "  },\n",
        "  {\n",
        "    'word': ['a', 'record', 'date', 'has', \"n't\", 'been', 'set', '.'],\n",
        "    'pos': ['DT', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'VBN', '.'],\n",
        "    'head': [3, 3, 7, 7, 7, 7, 0, 7],\n",
        "    'label': ['det', 'compound', 'nsubjpass', 'aux', 'neg', 'auxpass', 'root', 'punct']\n",
        "  },\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "After vectorization same data looks like this:\n",
        "\n",
        "39637 == ROOT is added in the begining of every word array\n",
        "\n",
        "87 == p_ROOT is added in the beginning of every POS array\n",
        "\n",
        "-1 is added in the beginning of every head and label array\n",
        "\n",
        "```\n",
        "[\n",
        "  {\n",
        "    'word': [39637, 89, 125, 258, 123, 115, 2742, 4197, 305, 90],\n",
        "    'pos': [87, 43, 44, 40, 59, 51, 44, 42, 48, 47],\n",
        "    'head': [-1, 3, 3, 6, 6, 6, 0, 6, 7, 6],\n",
        "    'label': [-1, 32, 10, 38, 11, 18, 0, 33, 34, 16]\n",
        "  },\n",
        "  {\n",
        "    'word': [39637, 93, 566, 1474, 120, 122, 149, 495, 90],\n",
        "    'pos': [87, 43, 40, 40, 54, 49, 55, 55, 47],\n",
        "    'head': [-1, 3, 3, 7, 7, 7, 7, 0, 7],\n",
        "    'label': [-1, 32, 7, 25, 11, 6, 20, 0, 16]\n",
        "  },\n",
        "  ...\n",
        "]\n"
      ]
    }
  ]
}