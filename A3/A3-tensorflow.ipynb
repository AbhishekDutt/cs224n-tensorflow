{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNcUWTUAvrpxbaBzDAWzLuV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhishekDutt/cs224n-tensorflow/blob/devlop/A3/A3-tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL0IP-MhjB7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "9f18d537-927b-46f9-955e-1d9bf55f5c32"
      },
      "source": [
        "# Download to /content/data/test.conll\n",
        "!wget https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/train.conll -P ./data\n",
        "!wget https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/dev.conll -P ./data\n",
        "!wget https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/test.conll -P ./data\n",
        "!wget https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/en-cw.txt -P ./data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-21 02:12:26--  https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/train.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30955799 (30M) [text/plain]\n",
            "Saving to: ‘./data/train.conll.1’\n",
            "\n",
            "\rtrain.conll.1         0%[                    ]       0  --.-KB/s               \rtrain.conll.1        22%[===>                ]   6.79M  33.9MB/s               \rtrain.conll.1       100%[===================>]  29.52M  75.0MB/s    in 0.4s    \n",
            "\n",
            "2020-06-21 02:12:26 (75.0 MB/s) - ‘./data/train.conll.1’ saved [30955799/30955799]\n",
            "\n",
            "--2020-06-21 02:12:28--  https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/dev.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1306509 (1.2M) [text/plain]\n",
            "Saving to: ‘./data/dev.conll.1’\n",
            "\n",
            "dev.conll.1         100%[===================>]   1.25M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-06-21 02:12:28 (13.8 MB/s) - ‘./data/dev.conll.1’ saved [1306509/1306509]\n",
            "\n",
            "--2020-06-21 02:12:31--  https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/test.conll\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1846748 (1.8M) [text/plain]\n",
            "Saving to: ‘./data/test.conll.1’\n",
            "\n",
            "test.conll.1        100%[===================>]   1.76M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-06-21 02:12:31 (14.6 MB/s) - ‘./data/test.conll.1’ saved [1846748/1846748]\n",
            "\n",
            "--2020-06-21 02:12:33--  https://raw.githubusercontent.com/AbhishekDutt/cs224n-tensorflow/devlop/A3/data/en-cw.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60531573 (58M) [text/plain]\n",
            "Saving to: ‘./data/en-cw.txt.1’\n",
            "\n",
            "en-cw.txt.1         100%[===================>]  57.73M  83.2MB/s    in 0.7s    \n",
            "\n",
            "2020-06-21 02:12:34 (83.2 MB/s) - ‘./data/en-cw.txt.1’ saved [60531573/60531573]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDRNgoGlWqvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a482c40c-016c-42f4-c53e-4d10cc7d84b5"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I2o9fig20Zt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "general_utils.py: General purpose utilities.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_minibatches(data, minibatch_size, shuffle=True):\n",
        "    \"\"\"\n",
        "    Iterates through the provided data one minibatch at at time. You can use this function to\n",
        "    iterate through data in minibatches as follows:\n",
        "\n",
        "        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n",
        "            ...\n",
        "\n",
        "    Or with multiple data sources:\n",
        "\n",
        "        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n",
        "            ...\n",
        "\n",
        "    Args:\n",
        "        data: there are two possible values:\n",
        "            - a list or numpy array\n",
        "            - a list where each element is either a list or numpy array\n",
        "        minibatch_size: the maximum number of items in a minibatch\n",
        "        shuffle: whether to randomize the order of returned data\n",
        "    Returns:\n",
        "        minibatches: the return value depends on data:\n",
        "            - If data is a list/array it yields the next minibatch of data.\n",
        "            - If data a list of lists/arrays it returns the next minibatch of each element in the\n",
        "              list. This can be used to iterate through multiple data sources\n",
        "              (e.g., features and labels) at the same time.\n",
        "\n",
        "    \"\"\"\n",
        "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n",
        "    data_size = len(data[0]) if list_data else len(data)\n",
        "    indices = np.arange(data_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
        "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
        "        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
        "            else _minibatch(data, minibatch_indices)\n",
        "\n",
        "\n",
        "def _minibatch(data, minibatch_idx):\n",
        "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
        "\n",
        "\n",
        "def test_all_close(name, actual, expected):\n",
        "    if actual.shape != expected.shape:\n",
        "        raise ValueError(\"{:} failed, expected output to have shape {:} but has shape {:}\"\n",
        "                         .format(name, expected.shape, actual.shape))\n",
        "    if np.amax(np.fabs(actual - expected)) > 1e-6:\n",
        "        raise ValueError(\"{:} failed, expected {:} but value is {:}\".format(name, expected, actual))\n",
        "    else:\n",
        "        print(name, \"passed!\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvaf3nQ5221-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "parser_utils.py: Utilities for training the dependency parser.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "from collections import Counter\n",
        "# from . general_utils import get_minibatches\n",
        "# from parser_transitions import minibatch_parse\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "P_PREFIX = '<p>:'\n",
        "L_PREFIX = '<l>:'\n",
        "UNK = '<UNK>'\n",
        "NULL = '<NULL>'\n",
        "ROOT = '<ROOT>'\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    language = 'english'\n",
        "    with_punct = True\n",
        "    unlabeled = True\n",
        "    lowercase = True\n",
        "    use_pos = True\n",
        "    use_dep = True\n",
        "    use_dep = use_dep and (not unlabeled)\n",
        "    data_path = './data'\n",
        "    train_file = 'train.conll'\n",
        "    dev_file = 'dev.conll'\n",
        "    test_file = 'test.conll'\n",
        "    embedding_file = './data/en-cw.txt'\n",
        "\n",
        "\n",
        "class Parser(object):\n",
        "    \"\"\"Contains everything needed for transition-based dependency parsing except for the model\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        root_labels = list([l for ex in dataset\n",
        "                           for (h, l) in zip(ex['head'], ex['label']) if h == 0])\n",
        "        counter = Counter(root_labels)\n",
        "        if len(counter) > 1:\n",
        "            logging.info('Warning: more than one root label')\n",
        "            logging.info(counter)\n",
        "        self.root_label = counter.most_common()[0][0]\n",
        "        deprel = [self.root_label] + list(set([w for ex in dataset\n",
        "                                               for w in ex['label']\n",
        "                                               if w != self.root_label]))\n",
        "        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)}\n",
        "        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n",
        "\n",
        "        config = Config()\n",
        "        self.unlabeled = config.unlabeled\n",
        "        self.with_punct = config.with_punct\n",
        "        self.use_pos = config.use_pos\n",
        "        self.use_dep = config.use_dep\n",
        "        self.language = config.language\n",
        "\n",
        "        if self.unlabeled:\n",
        "            trans = ['L', 'R', 'S']\n",
        "            self.n_deprel = 1\n",
        "        else:\n",
        "            trans = ['L-' + l for l in deprel] + ['R-' + l for l in deprel] + ['S']\n",
        "            self.n_deprel = len(deprel)\n",
        "\n",
        "        self.n_trans = len(trans)\n",
        "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
        "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
        "\n",
        "        # logging.info('Build dictionary for part-of-speech tags.')\n",
        "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n",
        "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
        "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
        "\n",
        "        # logging.info('Build dictionary for words.')\n",
        "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
        "                                  offset=len(tok2id)))\n",
        "        tok2id[UNK] = self.UNK = len(tok2id)\n",
        "        tok2id[NULL] = self.NULL = len(tok2id)\n",
        "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
        "\n",
        "        self.tok2id = tok2id\n",
        "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
        "\n",
        "        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n",
        "        self.n_tokens = len(tok2id)\n",
        "\n",
        "    def vectorize(self, examples):\n",
        "        vec_examples = []\n",
        "        for ex in examples:\n",
        "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
        "                                  else self.UNK for w in ex['word']]\n",
        "            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
        "                                   else self.P_UNK for w in ex['pos']]\n",
        "            head = [-1] + ex['head']\n",
        "            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
        "                            else -1 for w in ex['label']]\n",
        "            vec_examples.append({'word': word, 'pos': pos,\n",
        "                                 'head': head, 'label': label})\n",
        "        return vec_examples\n",
        "\n",
        "    def extract_features(self, stack, buf, arcs, ex):\n",
        "        if stack[0] == \"ROOT\":\n",
        "            stack[0] = 0\n",
        "\n",
        "        def get_lc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
        "\n",
        "        def get_rc(k):\n",
        "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
        "                          reverse=True)\n",
        "\n",
        "        p_features = []\n",
        "        l_features = []\n",
        "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
        "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
        "        if self.use_pos:\n",
        "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
        "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
        "\n",
        "        for i in range(2):\n",
        "            if i < len(stack):\n",
        "                k = stack[-i-1]\n",
        "                lc = get_lc(k)\n",
        "                rc = get_rc(k)\n",
        "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
        "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
        "\n",
        "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
        "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
        "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
        "\n",
        "                if self.use_pos:\n",
        "                    p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
        "                    p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
        "\n",
        "                if self.use_dep:\n",
        "                    l_features.append(ex['label'][lc[0]] if len(lc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rc[0]] if len(rc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][lc[1]] if len(lc) > 1 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rc[1]] if len(rc) > 1 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][llc[0]] if len(llc) > 0 else self.L_NULL)\n",
        "                    l_features.append(ex['label'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n",
        "            else:\n",
        "                features += [self.NULL] * 6\n",
        "                if self.use_pos:\n",
        "                    p_features += [self.P_NULL] * 6\n",
        "                if self.use_dep:\n",
        "                    l_features += [self.L_NULL] * 6\n",
        "\n",
        "        features += p_features + l_features\n",
        "        assert len(features) == self.n_features\n",
        "        return features\n",
        "\n",
        "    def get_oracle(self, stack, buf, ex):\n",
        "        if len(stack) < 2:\n",
        "            return self.n_trans - 1\n",
        "\n",
        "        i0 = stack[-1]\n",
        "        i1 = stack[-2]\n",
        "        h0 = ex['head'][i0]\n",
        "        h1 = ex['head'][i1]\n",
        "        l0 = ex['label'][i0]\n",
        "        l1 = ex['label'][i1]\n",
        "\n",
        "        if self.unlabeled:\n",
        "            if (i1 > 0) and (h1 == i0):\n",
        "                return 0\n",
        "            elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "                return 1\n",
        "            else:\n",
        "                return None if len(buf) == 0 else 2\n",
        "        else:\n",
        "            if (i1 > 0) and (h1 == i0):\n",
        "                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n",
        "            elif (i1 >= 0) and (h0 == i1) and \\\n",
        "                 (not any([x for x in buf if ex['head'][x] == i0])):\n",
        "                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n",
        "            else:\n",
        "                return None if len(buf) == 0 else self.n_trans - 1\n",
        "\n",
        "    def create_instances(self, examples):\n",
        "        all_instances = []\n",
        "        succ = 0\n",
        "        for id, ex in enumerate(examples):\n",
        "            n_words = len(ex['word']) - 1\n",
        "\n",
        "            # arcs = {(h, t, label)}\n",
        "            stack = [0]\n",
        "            buf = [i + 1 for i in range(n_words)]\n",
        "            arcs = []\n",
        "            instances = []\n",
        "            for i in range(n_words * 2):\n",
        "                gold_t = self.get_oracle(stack, buf, ex)\n",
        "                if gold_t is None:\n",
        "                    break\n",
        "                legal_labels = self.legal_labels(stack, buf)\n",
        "                assert legal_labels[gold_t] == 1\n",
        "                instances.append((self.extract_features(stack, buf, arcs, ex),\n",
        "                                  legal_labels, gold_t))\n",
        "                if gold_t == self.n_trans - 1:\n",
        "                    stack.append(buf[0])\n",
        "                    buf = buf[1:]\n",
        "                elif gold_t < self.n_deprel:\n",
        "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
        "                    stack = stack[:-2] + [stack[-1]]\n",
        "                else:\n",
        "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
        "                    stack = stack[:-1]\n",
        "            else:\n",
        "                succ += 1\n",
        "                all_instances += instances\n",
        "\n",
        "        return all_instances\n",
        "\n",
        "    def legal_labels(self, stack, buf):\n",
        "        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n",
        "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n",
        "        labels += [1] if len(buf) > 0 else [0]\n",
        "        return labels\n",
        "\n",
        "    def parse(self, dataset, eval_batch_size=5000):\n",
        "        sentences = []\n",
        "        sentence_id_to_idx = {}\n",
        "        for i, example in enumerate(dataset):\n",
        "            n_words = len(example['word']) - 1\n",
        "            sentence = [j + 1 for j in range(n_words)]\n",
        "            sentences.append(sentence)\n",
        "            sentence_id_to_idx[id(sentence)] = i\n",
        "\n",
        "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
        "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
        "\n",
        "        UAS = all_tokens = 0.0\n",
        "        with tqdm(total=len(dataset)) as prog:\n",
        "            for i, ex in enumerate(dataset):\n",
        "                head = [-1] * len(ex['word'])\n",
        "                for h, t, in dependencies[i]:\n",
        "                    head[t] = h\n",
        "                for pred_h, gold_h, gold_l, pos in \\\n",
        "                        zip(head[1:], ex['head'][1:], ex['label'][1:], ex['pos'][1:]):\n",
        "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
        "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
        "                        if (self.with_punct) or (not punct(self.language, pos_str)):\n",
        "                            UAS += 1 if pred_h == gold_h else 0\n",
        "                            all_tokens += 1\n",
        "                prog.update(i + 1)\n",
        "        UAS /= all_tokens\n",
        "        return UAS, dependencies\n",
        "\n",
        "\n",
        "class ModelWrapper(object):\n",
        "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
        "        self.parser = parser\n",
        "        self.dataset = dataset\n",
        "        self.sentence_id_to_idx = sentence_id_to_idx\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n",
        "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
        "                for p in partial_parses]\n",
        "        mb_x = np.array(mb_x).astype('int32')\n",
        "        # mb_x = torch.from_numpy(mb_x).long()\n",
        "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
        "        # pred = self.parser.model(mb_x)\n",
        "        pred = self.parser.model(mb_x, training=False)  ## [TODO: model.evaluate or model.predict might be better]\n",
        "        # pred = pred.detach().numpy()\n",
        "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
        "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
        "        return pred\n",
        "\n",
        "\n",
        "def read_conll(in_file, lowercase=False, max_example=None):\n",
        "    examples = []\n",
        "    with open(in_file) as f:\n",
        "        word, pos, head, label = [], [], [], []\n",
        "        for line in f.readlines():\n",
        "            sp = line.strip().split('\\t')\n",
        "            if len(sp) == 10:\n",
        "                if '-' not in sp[0]:\n",
        "                    word.append(sp[1].lower() if lowercase else sp[1])\n",
        "                    pos.append(sp[4])\n",
        "                    head.append(int(sp[6]))\n",
        "                    label.append(sp[7])\n",
        "            elif len(word) > 0:\n",
        "                examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
        "                word, pos, head, label = [], [], [], []\n",
        "                if (max_example is not None) and (len(examples) == max_example):\n",
        "                    break\n",
        "        if len(word) > 0:\n",
        "            examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
        "    return examples\n",
        "\n",
        "\n",
        "def build_dict(keys, n_max=None, offset=0):\n",
        "    count = Counter()\n",
        "    for key in keys:\n",
        "        count[key] += 1\n",
        "    ls = count.most_common() if n_max is None \\\n",
        "        else count.most_common(n_max)\n",
        "\n",
        "    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n",
        "\n",
        "\n",
        "def punct(language, pos):\n",
        "    if language == 'english':\n",
        "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
        "    elif language == 'chinese':\n",
        "        return pos == 'PU'\n",
        "    elif language == 'french':\n",
        "        return pos == 'PUNC'\n",
        "    elif language == 'german':\n",
        "        return pos in [\"$.\", \"$,\", \"$[\"]\n",
        "    elif language == 'spanish':\n",
        "        # http://nlp.stanford.edu/software/spanish-faq.shtml\n",
        "        return pos in [\"f0\", \"faa\", \"fat\", \"fc\", \"fd\", \"fe\", \"fg\", \"fh\",\n",
        "                       \"fia\", \"fit\", \"fp\", \"fpa\", \"fpt\", \"fs\", \"ft\",\n",
        "                       \"fx\", \"fz\"]\n",
        "    elif language == 'universal':\n",
        "        return pos == 'PUNCT'\n",
        "    else:\n",
        "        raise ValueError('language: %s is not supported.' % language)\n",
        "\n",
        "\n",
        "def minibatches(data, batch_size):\n",
        "    x = np.array([d[0] for d in data])\n",
        "    y = np.array([d[2] for d in data])\n",
        "    one_hot = np.zeros((y.size, 3))\n",
        "    one_hot[np.arange(y.size), y] = 1\n",
        "    return get_minibatches([x, one_hot], batch_size)\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(reduced=True):\n",
        "    config = Config()\n",
        "\n",
        "    print(\"Loading data...\",)\n",
        "    start = time.time()\n",
        "    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n",
        "                           lowercase=config.lowercase)\n",
        "    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n",
        "                         lowercase=config.lowercase)\n",
        "    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n",
        "                          lowercase=config.lowercase)\n",
        "    if reduced:\n",
        "        train_set = train_set[:1000]\n",
        "        dev_set = dev_set[:500]\n",
        "        test_set = test_set[:500]\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Building parser...\",)\n",
        "    start = time.time()\n",
        "    parser = Parser(train_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Loading pretrained embeddings...\",)\n",
        "    start = time.time()\n",
        "    word_vectors = {}\n",
        "    for line in open(config.embedding_file).readlines():\n",
        "        sp = line.strip().split()\n",
        "        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n",
        "    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
        "\n",
        "    for token in parser.tok2id:\n",
        "        i = parser.tok2id[token]\n",
        "        if token in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token]\n",
        "        elif token.lower() in word_vectors:\n",
        "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Vectorizing data...\",)\n",
        "    start = time.time()\n",
        "    train_set = parser.vectorize(train_set)\n",
        "    dev_set = parser.vectorize(dev_set)\n",
        "    test_set = parser.vectorize(test_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    print(\"Preprocessing training data...\",)\n",
        "    start = time.time()\n",
        "    train_examples = parser.create_instances(train_set)\n",
        "    print(\"took {:.2f} seconds\".format(time.time() - start))\n",
        "\n",
        "    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns9jleml29PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "parser_transitions.py: Algorithms for completing partial parsess.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "\n",
        "class PartialParse(object):\n",
        "    def __init__(self, sentence):\n",
        "        \"\"\"Initializes this partial parse.\n",
        "\n",
        "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
        "                                        Your code should not modify the sentence.\n",
        "        \"\"\"\n",
        "        # The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.\n",
        "        self.sentence = sentence\n",
        "\n",
        "        ### YOUR CODE HERE (3 Lines)\n",
        "        ### Your code should initialize the following fields:\n",
        "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
        "        ###                 last element of the list.\n",
        "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
        "        ###                  buffer as the first item of the list\n",
        "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
        "        ###             tuples where each tuple is of the form (head, dependent).\n",
        "        ###             Order for this list doesn't matter.\n",
        "        ###\n",
        "        ### Note: The root token should be represented with the string \"ROOT\"\n",
        "        ###\n",
        "        self.stack = [\"ROOT\"]\n",
        "        self.buffer = sentence.copy()\n",
        "        self.dependencies = []\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "\n",
        "    def parse_step(self, transition):\n",
        "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
        "\n",
        "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
        "                                left-arc, and right-arc transitions. You can assume the provided\n",
        "                                transition is a legal transition.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE (~7-10 Lines)\n",
        "        ### TODO:\n",
        "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
        "        ###     described in the pdf handout:\n",
        "        ###         1. Shift\n",
        "        ###         2. Left Arc\n",
        "        ###         3. Right Arc\n",
        "        if transition == \"S\":\n",
        "            self.stack.append(self.buffer[0])\n",
        "            del self.buffer[0]\n",
        "        if transition == \"LA\":\n",
        "            self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
        "            del self.stack[-2]\n",
        "        if transition == \"RA\":\n",
        "            self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
        "            del self.stack[-1]\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def parse(self, transitions):\n",
        "        \"\"\"Applies the provided transitions to this PartialParse\n",
        "\n",
        "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
        "\n",
        "        @return dsependencies (list of string tuples): The list of dependencies produced when\n",
        "                                                        parsing the sentence. Represented as a list of\n",
        "                                                        tuples where each tuple is of the form (head, dependent).\n",
        "        \"\"\"\n",
        "        for transition in transitions:\n",
        "            self.parse_step(transition)\n",
        "        return self.dependencies\n",
        "\n",
        "\n",
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
        "\n",
        "    @param sentences (list of list of str): A list of sentences to be parsed\n",
        "                                            (each sentence is a list of words and each word is of type string)\n",
        "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
        "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
        "                                returns a list of transitions predicted for each parse. That is, after calling\n",
        "                                    transitions = model.predict(partial_parses)\n",
        "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
        "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
        "\n",
        "\n",
        "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
        "                                                    list for a parsed sentence. Ordering should be the\n",
        "                                                    same as in sentences (i.e., dependencies[i] should\n",
        "                                                    contain the parse for sentences[i]).\n",
        "    \"\"\"\n",
        "    dependencies = []\n",
        "\n",
        "    ### YOUR CODE HERE (~8-10 Lines)\n",
        "    ### TODO:\n",
        "    ###     Implement the minibatch parse algorithm as described in the pdf handout\n",
        "    ###\n",
        "    ###     Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g.\n",
        "    ###                 unfinished_parses = partial_parses[:].\n",
        "    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
        "    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n",
        "    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n",
        "    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n",
        "    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n",
        "    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n",
        "    ###             is being accessed by `partial_parses` and may cause your code to crash.\n",
        "    \n",
        "    partial_parses = [PartialParse(sentence) for sentence in sentences]\n",
        "    unfinished_parses = partial_parses[:]\n",
        "    while unfinished_parses:\n",
        "        minibatch = unfinished_parses[:batch_size]\n",
        "        transitions = model.predict(minibatch)\n",
        "        for pp, transition in zip(minibatch, transitions):\n",
        "            pp.parse_step(transition)\n",
        "            if len(pp.buffer) == 0 and len(pp.stack) == 1:\n",
        "                unfinished_parses.remove(pp)\n",
        "\n",
        "    dependencies = [pp.dependencies for pp in partial_parses]\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return dependencies\n",
        "\n",
        "\n",
        "def test_step(name, transition, stack, buf, deps,\n",
        "              ex_stack, ex_buf, ex_deps):\n",
        "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
        "    pp = PartialParse([])\n",
        "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
        "\n",
        "    pp.parse_step(transition)\n",
        "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
        "    assert stack == ex_stack, \\\n",
        "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
        "    assert buf == ex_buf, \\\n",
        "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "    print(\"{:} test passed!\".format(name))\n",
        "\n",
        "\n",
        "def test_parse_step():\n",
        "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
        "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
        "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
        "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
        "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
        "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
        "\n",
        "\n",
        "def test_parse():\n",
        "    \"\"\"Simple tests for the PartialParse.parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
        "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
        "    dependencies = tuple(sorted(dependencies))\n",
        "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
        "    assert dependencies == expected,  \\\n",
        "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
        "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
        "        \"parse test failed: the input sentence should not be modified\"\n",
        "    print(\"parse test passed!\")\n",
        "\n",
        "\n",
        "class DummyModel(object):\n",
        "    \"\"\"Dummy model for testing the minibatch_parse function\n",
        "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
        "    the sentence is \"right\", \"left\" if otherwise.\n",
        "    \"\"\"\n",
        "    def predict(self, partial_parses):\n",
        "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]\n",
        "\n",
        "\n",
        "def test_dependencies(name, deps, ex_deps):\n",
        "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
        "    deps = tuple(sorted(deps))\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "\n",
        "\n",
        "def test_minibatch_parse():\n",
        "    \"\"\"Simple tests for the minibatch_parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
        "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
        "                 [\"left\", \"arcs\", \"only\"],\n",
        "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[1],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[2],\n",
        "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[3],\n",
        "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
        "    print(\"minibatch_parse test passed!\")\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     args = sys.argv\n",
        "#     if len(args) != 2:\n",
        "#         raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n",
        "#     elif args[1] == \"part_c\":\n",
        "#         test_parse_step()\n",
        "#         test_parse()\n",
        "#     elif args[1] == \"part_d\":\n",
        "#         test_minibatch_parse()\n",
        "#     else:\n",
        "#         raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-nwznyL3BnK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c129f01c-9b63-49e5-915f-b24998d253c2"
      },
      "source": [
        "test_parse_step()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SHIFT test passed!\n",
            "LEFT-ARC test passed!\n",
            "RIGHT-ARC test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97wV_LMH3EoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d4ad61a-96ed-4ef0-c419-b4e6b34bb836"
      },
      "source": [
        "test_parse()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parse test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86C7HBmI3Hoz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc267d09-6bf3-4bb0-f8d8-406b4e04273c"
      },
      "source": [
        "test_minibatch_parse()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minibatch_parse test passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzy1VO3A3Jt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# import tensorflow.keras as keras\n",
        "class ParserModel(tf.keras.Model):\n",
        "    def __init__(self, embeddings, n_features=36, hidden_size=200, n_classes=3, \n",
        "                 dropout_prob=0.5):\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
        "        # self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        # ---->\n",
        "        # self.pretrained_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=300, \n",
        "        #                               input_length=Length_of_input_sequences, \n",
        "        #                               embeddings_initializer=matrix_of_pretrained_weights)\n",
        "        \n",
        "        ## Note: Embedding layer is essentially a lookup table\n",
        "        # embedding shape = (num_of_tokens, embed_size)\n",
        "        # Shape of the expected input = (batch, n_features)\n",
        "        # output shape = (batch, n_features, embed_size) - OR - (batch, n_features * emebd_size)  \n",
        "        self.pretrained_embeddings = tf.keras.layers.Embedding(input_dim=embeddings.shape[0], output_dim=self.embed_size,\n",
        "                              input_length=n_features, name='embedding', trainable=False)\n",
        "        self.pretrained_embeddings.build(input_shape=(1,)) # the input_shape here has no effect in the build function\n",
        "        # self.pretrained_embeddings.build() # the input_shape here has no effect in the build function\n",
        "        self.pretrained_embeddings.set_weights([embeddings])\n",
        "        # print(\" == embeddings == \")\n",
        "        # print(embeddings.shape)\n",
        "        # print(embeddings)\n",
        "        # print(embeddings[85])\n",
        "        # print(embeddings[86])\n",
        "        # print(embeddings[87])\n",
        "        # print(embeddings[88])\n",
        "        # print(self.pretrained_embeddings.get_weights())\n",
        "        # <----\n",
        "\n",
        "        ### YOUR CODE HERE (~5 Lines)\n",
        "\n",
        "        # self.embed_to_hidden = nn.Linear(in_features=n_features*self.embed_size, out_features=hidden_size)\n",
        "        # nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain=1)\n",
        "        # self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        # self.hidden_to_logits = nn.Linear(in_features=hidden_size, out_features=n_classes)\n",
        "        # nn.init.xavier_uniform_(self.hidden_to_logits.weight, gain=1)\n",
        "        \n",
        "        # ---->\n",
        "        # Glorot normal:\n",
        "        # https://stackoverflow.com/questions/43284047/what-is-the-default-kernel-initializer-in-tf-layers-conv2d-and-tf-layers-dense\n",
        "        # init = tf.initializers.GlorotUniform()\n",
        "        # tf.random.set_seed(0)\n",
        "        self.embed_to_hidden = tf.keras.layers.Dense(units=hidden_size, \n",
        "                                     kernel_initializer='glorot_uniform')\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=dropout_prob)\n",
        "        self.hidden_to_logits = tf.keras.layers.Dense(units=n_classes,\n",
        "                                     kernel_initializer='glorot_uniform')\n",
        "        # <----\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def embedding_lookup(self, t):\n",
        "        ### YOUR CODE HERE (~1-3 Lines)\n",
        "        # x = self.pretrained_embeddings(t).view(t.size(0), self.n_features * self.embed_size)\n",
        "        # ---->\n",
        "        # print(\" == t == \")\n",
        "        # print(t.shape)\n",
        "        # print(t)\n",
        "        x = self.pretrained_embeddings(t)\n",
        "        x = tf.reshape(x, [t.shape[0], self.n_features*self.embed_size])\n",
        "        # print(\" == x == \")\n",
        "        # print(x.shape, x.shape[0])\n",
        "        # print(tf.size(x))\n",
        "        # print(x)\n",
        "        # print(\"-=-=-=\")\n",
        "        # print(t[0][0])\n",
        "        # print(self.pretrained_embeddings.get_weights()[0][t[0][0]])\n",
        "        # print(self.pretrained_embeddings.get_weights()[0][t[0][35]])\n",
        "        # print(self.pretrained_embeddings.get_weights()[0][t[0][36]])\n",
        "        # <----\n",
        "        ### END YOUR CODE\n",
        "        return x\n",
        "\n",
        "\n",
        "    def call(self, t):\n",
        "        ###  YOUR CODE HERE (~3-5 lines)\n",
        "        # embeddings = self.embedding_lookup(t)\n",
        "        # hidden_state = self.embed_to_hidden(embeddings)\n",
        "        # hidden_relu = torch.relu(hidden_state)\n",
        "        # hidden_dropout = self.dropout(hidden_relu)\n",
        "        # logits = self.hidden_to_logits(hidden_dropout)\n",
        "        # ---->\n",
        "        embeddings = self.embedding_lookup(t)\n",
        "        hidden_state = self.embed_to_hidden(embeddings)\n",
        "        hidden_relu = tf.nn.relu(hidden_state)\n",
        "        hidden_dropout = self.dropout(hidden_relu)\n",
        "        logits = self.hidden_to_logits(hidden_dropout)\n",
        "        # <----\n",
        "        ### END YOUR CODE\n",
        "        return logits\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaYevr6o3Qjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d588227b-94fd-4ff1-8a9a-3db34077d8c1"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2018-19: Homework 3\n",
        "run.py: Run the dependency parser.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from parser_model import ParserModel\n",
        "# from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n",
        "\n",
        "# -----------------\n",
        "# Primary Functions\n",
        "# -----------------\n",
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \"\"\" Train the neural dependency parser.\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param output_path (str): Path to which model weights and results are written.\n",
        "    @param batch_size (int): Number of examples in a single batch\n",
        "    @param n_epochs (int): Number of training epochs\n",
        "    @param lr (float): Learning rate\n",
        "    \"\"\"\n",
        "    best_dev_UAS = 0\n",
        "\n",
        "\n",
        "    ### YOUR CODE HERE (~2-7 lines)\n",
        "    ### TODO:\n",
        "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
        "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func`\n",
        "    ###\n",
        "    ### Hint: Use `parser.model.parameters()` to pass optimizer\n",
        "    ###       necessary parameters to tune.\n",
        "    ### Please see the following docs for support:\n",
        "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
        "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "\n",
        "    # optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
        "    # loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-08, name='Adam')\n",
        "    # loss_func = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n",
        "    loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            # torch.save(parser.model.state_dict(), output_path)\n",
        "            parser.model.save_weights(output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \"\"\" Train the neural dependency parser for single epoch.\n",
        "\n",
        "    Note: In PyTorch we can signify train versus test and automatically have\n",
        "    the Dropout Layer applied and removed, accordingly, by specifying\n",
        "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
        "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
        "    @param batch_size (int): batch size\n",
        "    @param lr (float): learning rate\n",
        "\n",
        "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
        "    \"\"\"\n",
        "    # parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            # optimizer.zero_grad()   # remove any baggage in the optimizer\n",
        "            loss = 0. # store loss for this batch here\n",
        "            # train_x = torch.from_numpy(train_x).long()\n",
        "            # train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
        "\n",
        "            ### YOUR CODE HERE (~5-10 lines)\n",
        "            ### TODO:\n",
        "            ###      1) Run train_x forward through model to produce `logits`\n",
        "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
        "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
        "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
        "            ###         are the predictions (y^ from the PDF).\n",
        "            ###      3) Backprop losses\n",
        "            ###      4) Take step with the optimizer\n",
        "            ### Please see the following docs for support:\n",
        "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
        "\n",
        "            # logits = parser.model(train_x)\n",
        "            # loss = loss_func(logits, train_y)\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "              logits = parser.model(train_x, training=True) # <- can be above gradient tape too\n",
        "              loss = loss_func(train_y, logits)\n",
        "              # print(\"loss - \", loss)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            ### END YOUR CODE\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.numpy())\n",
        "\n",
        "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    # parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Note: Set debug to False, when training on entire corpus\n",
        "    debug = True\n",
        "    # debug = False\n",
        "    print(torch.__version__)\n",
        "    # assert(torch.__version__ == \"1.0.0\"),  \"Please install torch version 1.0.0\"\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"INITIALIZING\")\n",
        "    print(80 * \"=\")\n",
        "    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n",
        "\n",
        "    start = time.time()\n",
        "    model = ParserModel(embeddings)\n",
        "    \n",
        "    use_cuda = True\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "\n",
        "    parser.model = model\n",
        "\n",
        "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"TRAINING\")\n",
        "    print(80 * \"=\")\n",
        "    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "    output_path = output_dir + \"model.weights\"\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
        "\n",
        "    if not debug:\n",
        "        print(80 * \"=\")\n",
        "        print(\"TESTING\")\n",
        "        print(80 * \"=\")\n",
        "        print(\"Restoring the best model weights found on the dev set\")\n",
        "        # parser.model.load_state_dict(torch.load(output_path))\n",
        "        parser.model.load_weights(output_path)\n",
        "        print(\"Final evaluation on test set\",)\n",
        "        # parser.model.eval()\n",
        "        UAS, dependencies = parser.parse(test_data)\n",
        "        print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "        print(\"Done!\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "================================================================================\n",
            "INITIALIZING\n",
            "================================================================================\n",
            "Loading data...\n",
            "took 2.07 seconds\n",
            "Building parser...\n",
            "took 0.04 seconds\n",
            "Loading pretrained embeddings...\n",
            "took 2.24 seconds\n",
            "Vectorizing data...\n",
            "took 0.25 seconds\n",
            "Preprocessing training data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "took 1.20 seconds\n",
            "took 0.19 seconds\n",
            "\n",
            "================================================================================\n",
            "TRAINING\n",
            "================================================================================\n",
            "Epoch 1 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:03<00:00, 15.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.1377665922045708\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11403998.09it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.12\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 2 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.575421031564474\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 8169959.66it/s]       \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 4.17\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 3 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 2.4124391997853913\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 12722478.35it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 3.68\n",
            "\n",
            "Epoch 4 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.0721511219938595\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 10606646.13it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 3.78\n",
            "\n",
            "Epoch 5 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.60it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 0.9701139765481154\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 12711088.49it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 3.92\n",
            "\n",
            "Epoch 6 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 2.4564042687416077\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 12396153.19it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 3.72\n",
            "\n",
            "Epoch 7 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 5.195825840036075\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11778847.00it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 6.11\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 8 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 2.2379015435775123\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 12705247.56it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 9.24\n",
            "New best dev UAS! Saving model.\n",
            "\n",
            "Epoch 9 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.53it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.1870997175574303\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 10269907.45it/s]      \n",
            "  0%|          | 0/48 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 5.53\n",
            "\n",
            "Epoch 10 out of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 48/48 [00:02<00:00, 16.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Train Loss: 1.1207772319515545\n",
            "Evaluating on dev set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "125250it [00:00, 11371659.98it/s]      "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "- dev UAS: 6.54\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5UhSO-sawOU",
        "colab_type": "text"
      },
      "source": [
        "### Train Data\n",
        "So what does the training data looks like?\n",
        "\n",
        "Training data is a tab seprated text file with 10 columns.\n",
        "\n",
        "Following is the screenshot of a part of the train.conll showing two sentences:\n",
        "1.   The new rate will be payable Feb. 15.\n",
        "2.   A record date hasn't been set.\n",
        "\n",
        "![train.conll](https://github.com/AbhishekDutt/cs224n-tensorflow/raw/devlop/A3/screenshots/train.conll.1.annotated.png)\n",
        "\n",
        "TODO: Add details about what each column means\n",
        "\n",
        "Each of the train, dev and test data is converted into a list of dictionries by read_conll() function.\n",
        "```\n",
        "[\n",
        "  {\n",
        "    'word': ['the', 'new', 'rate', 'will', 'be', 'payable', 'feb.', '15', '.'], \n",
        "    'pos': ['DT', 'JJ', 'NN', 'MD', 'VB', 'JJ', 'NNP', 'CD', '.'], \n",
        "    'head': [3, 3, 6, 6, 6, 0, 6, 7, 6], \n",
        "    'label': ['det', 'amod', 'nsubj', 'aux', 'cop', 'root', 'nmod:tmod', 'nummod', 'punct']\n",
        "  },\n",
        "  {\n",
        "    'word': ['a', 'record', 'date', 'has', \"n't\", 'been', 'set', '.'],\n",
        "    'pos': ['DT', 'NN', 'NN', 'VBZ', 'RB', 'VBN', 'VBN', '.'],\n",
        "    'head': [3, 3, 7, 7, 7, 7, 0, 7],\n",
        "    'label': ['det', 'compound', 'nsubjpass', 'aux', 'neg', 'auxpass', 'root', 'punct']\n",
        "  },\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "After vectorization same data looks like this:\n",
        "\n",
        "39637 == ROOT is added in the begining of every word array\n",
        "\n",
        "87 == p_ROOT is added in the beginning of every POS array\n",
        "\n",
        "-1 is added in the beginning of every head and label array\n",
        "\n",
        "```\n",
        "[\n",
        "  {\n",
        "    'word': [39637, 89, 125, 258, 123, 115, 2742, 4197, 305, 90],\n",
        "    'pos': [87, 43, 44, 40, 59, 51, 44, 42, 48, 47],\n",
        "    'head': [-1, 3, 3, 6, 6, 6, 0, 6, 7, 6],\n",
        "    'label': [-1, 32, 10, 38, 11, 18, 0, 33, 34, 16]\n",
        "  },\n",
        "  {\n",
        "    'word': [39637, 93, 566, 1474, 120, 122, 149, 495, 90],\n",
        "    'pos': [87, 43, 40, 40, 54, 49, 55, 55, 47],\n",
        "    'head': [-1, 3, 3, 7, 7, 7, 7, 0, 7],\n",
        "    'label': [-1, 32, 7, 25, 11, 6, 20, 0, 16]\n",
        "  },\n",
        "  ...\n",
        "]\n"
      ]
    }
  ]
}