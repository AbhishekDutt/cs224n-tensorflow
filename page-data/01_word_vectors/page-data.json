{"componentChunkName":"component---src-templates-docs-js","path":"/01_word_vectors","result":{"data":{"site":{"siteMetadata":{"title":"CS224n Natural Language Processing Tutorial | Abhishek Dutt","docsLocation":"https://abhishekdutt.github.io/cs224n-tensorflow/"}},"mdx":{"fields":{"id":"0bb9d01b-0aa6-50f3-9051-d344efb5d649","title":"Word Vectors","slug":"/01_word_vectors"},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Word Vectors\",\n  \"metaTitle\": \"Lecture 1 - Word Vectors\",\n  \"metaDescription\": \"All about word vectors\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"NLP Tasks:\\nFollowing are few of the common NLP tasks:\\n1\\n2\\n3\\n4\"), mdx(\"h1\", null, \"Word Vectors:\"), mdx(\"p\", null, \"Word vectors are a mathematical representation of written words. \", mdx(\"br\", null), \"\\nThey are needed as we can't just feed english words into mathematical models directly. \", mdx(\"br\", null), \"\\nEssentially word vectors are just a mapping from a word to a vector (hence the name Word Vector). \", mdx(\"br\", null)), mdx(\"p\", null, \"Many different ways of creating these Word Vector mappings have been developed. \", mdx(\"br\", null), \"\\nAs an example, here's what word vectors for the same word \\\"hotel\\\" from two different methods may look like: \", mdx(\"br\", null), \"\\nhotel -> \", \"[0 0 0 0 0 0 1 0 0 0 0]\", \" (10, 1) \", mdx(\"br\", null), \"\\nhotel -> \", \"[2.32 0.123 -0.645 6.231 8.211]\", \" (5, 1) \", mdx(\"br\", null)), mdx(\"p\", null, \"Few of the popular word vectors are:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Count based Word Vectors:\", mdx(\"ol\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"One hot vector\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Word document matrix\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Window based co-occurence matrix\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"TF-IDF\"))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Iteration based Word embeddings:\", mdx(\"ol\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Word2Vec\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"GLoVe\")))), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"PS: When Word Vectors are generated by iteration based methods, they are also called Word embeddings.\")), mdx(\"p\", null, \"Although newer and better methods (e.g. FastText, BERT, GPT-3) are being published regularly, Word2Vec is a great way to learn about the Word embeddings and the workflow of training NLP models.\"), mdx(\"hr\", null), mdx(\"p\", null, \"Assuming you already have some vague idea what is a word vector, if you dont this is a word vector:\"), mdx(\"p\", null, \"<TODO: Display a word vector>\"), mdx(\"p\", null, \"Word2Vec is the method by which we get array of numbers corresponding to a list of words.\\ni.e. Every word in a fixed vocablulary is represnedted by a vector.\"), mdx(\"p\", null, \"A list of such word vectors is called a word embedding, coz these numbers embed the meaning of the word in the vector.\"), mdx(\"h3\", null, \"Word2vec paper suggested 2 methods:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Continuous Bag Of Words: \", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Go through each poition 't' in the text, which has a center word 'c' and context (\\\"outside\\\") words \\\"o\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Use the similarity of the word vectors for c and o to calculate the probability of o given c (or vice versa in case of Skip gram)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Keep adjusting the word vectors to maximize this probability\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Skip-Gram method\")), mdx(\"p\", null, \"History:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Word2vec (Mikolov et al. 2013) is a framework for learning word vectors. \", \"[This is the link to the original paper, go ahead take a look]\")));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#word-vectors","title":"Word Vectors:","items":[{"items":[{"url":"#word2vec-paper-suggested-2-methods","title":"Word2vec paper suggested 2 methods:"}]}]}]},"parent":{"__typename":"File","relativePath":"01_word_vectors.md"},"frontmatter":{"metaTitle":"Lecture 1 - Word Vectors","metaDescription":"All about word vectors"}},"allMdx":{"edges":[{"node":{"fields":{"slug":"/01_word_vectors","title":"Word Vectors"}}},{"node":{"fields":{"slug":"/05_linguistic_structure_dependency_parsing","title":"05. Linguistic Structure: Dependency Parsing"}}},{"node":{"fields":{"slug":"/01_word_vectors/02_assignment1","title":"Assignment 1"}}},{"node":{"fields":{"slug":"/03_word_window","title":"03. Word Window Classification, Neural Networks, and Matrix Calculus"}}},{"node":{"fields":{"slug":"/07_vanishing_gradients_fancy_rnn","title":"07. Vanishing Gradients and Fancy RNNs"}}},{"node":{"fields":{"slug":"/codeblock","title":"Syntax Highlighting"}}},{"node":{"fields":{"slug":"/06_language_models_rnn","title":"06. The probability of a sentence? Recurrent Neural Networks and Language Models"}}},{"node":{"fields":{"slug":"/15_natural_language_generation","title":"15. Natural Language Generation"}}},{"node":{"fields":{"slug":"/extra_stuff","title":"Extra Stuff"}}},{"node":{"fields":{"slug":"/","title":"Welcome"}}},{"node":{"fields":{"slug":"/codeblock/1-index","title":"Sub Page"}}},{"node":{"fields":{"slug":"/01_word_vectors/01_count_based","title":"Count Based Word Vectors"}}},{"node":{"fields":{"slug":"/01_word_vectors/03_word2vec","title":"Word2Vec"}}},{"node":{"fields":{"slug":"/00_toc","title":"Table of Contents"}}}]}},"pageContext":{"id":"0bb9d01b-0aa6-50f3-9051-d344efb5d649"}}}