{"componentChunkName":"component---src-templates-docs-js","path":"/15_natural_language_generation","result":{"data":{"site":{"siteMetadata":{"title":"CS224n Natural Language Processing Tutorial | Abhishek Dutt","docsLocation":"https://abhishekdutt.github.io/cs224n-tensorflow/"}},"mdx":{"fields":{"id":"f669c146-88cb-5ecf-a26c-f781f248682c","title":"15. Natural Language Generation","slug":"/15_natural_language_generation"},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"15. Natural Language Generation\",\n  \"metaTitle\": \"This is the title tag of this page 15\",\n  \"metaDescription\": \"This is the meta description\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", null, \"Plan:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Recap what we already know about NLG\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"More on decoding algorithms\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"NLG tasks and neural approaches to them\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"BLG evaluation: a tricky situation\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Concluding thought on NLG research, current trends and the future\")), mdx(\"h3\", null, \"What is NLG:\"), mdx(\"p\", null, \"NLG refers to any setting in which we generate new text.\"), mdx(\"p\", null, \"NLG is a subcomponent of:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Machine Translation\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"(Abstractive) Summarizaiton\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Dialogue (chit-chat and task-based)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Creative writing: Storytelling, poetry-generation\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Freeform Question Answering (i.e. anser is generated, not extracted from text or knowledge base)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Image captioning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"etc.\")), mdx(\"h2\", null, \"1. Recap what we already know about NLG\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Language Modeling: the task of \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"predicting the next word\"), \", given the words so far:\\nP(yt|y1,...,yt-1)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A system that produces this probability distribution is called Language Model\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Contitional Language Modeling: the task of predicting the next word, given the words so far, \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"and also some other input x\"), \"\\nP(yt|y1,...,yt-1,x)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Examples of Conditional Language modeling tasks:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Machine Translation (x=source sentence, y=trget sentence)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Summarization (x=input text, y=summarized text)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Dialogue (x=dialogue history, y=next utterance)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"etc.\")))), mdx(\"h3\", null, \"Recap: training a (conditional) RNN-LM:\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1035px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/cs224n-tensorflow/static/8e62c1e1ce531db0d16f543189fdd7db/aa440/15_RNN.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"74.90347490347492%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABcSAAAXEgFnn9JSAAACz0lEQVQ4y21U247bNhD1/39KHwIELbZA0AZBEaANstl643WyzdqWLdu6i6KuvIonI3JzeSiB4xkOyZnD4ZFXxgJsAAYJTArgU4DQIdYJh6JzaEaHmvblrfOoegdpQMPBuYBlrAbp8PfO4Z+9w/bq8P7g8PaLw23ksDk7vIuAD2dgmwCbi8MD2fsr8CmFL/LzWJKulh9lHAZiMimqqh2xo7kMDHipYBhRdXM4JQxcI4A5zHVnIEoBq2xgiIWqpxuqzdYiS1PkWepjYz6ARQ36dsAkJKqYodoxdE0PKRWqYwUeccx6fk74P6NpORhvvM97g6ySyIhlwTXOhUBRKyQ1Je80jsmIutE/rmyl9ZQ9KgFJBzUd1C1tMjOa3uLKDC6VQcIsolwhayzi0iDnFrtUe/s94UJ1zCZMhcRUSgiqPhbBOirGuEFaa1wrSsSIUSaRE6O4UChpbZ8Isvr5VejKKk1D78QIp6jZavJwcvTxepixvUj8l2iP7Zl8YvV41Xgie3+S2GXaZ8sFx2r89BnzOODu5g7v/njE5s0W61drrF8/4FIDt4cOLz7E+PU+we+bDC9JM7+tr7j5mOGGYr+Qpu5OOWYYPLZnrGQcw2qNOrqCXQq01xxNnKJLSsx0hV4IZG2NjNcoe46kKVG0jMTeeHupc/CxhSVZHYaUdEgysaQpO1sYaygJ6VBJDGO4spQCQ9dDK+WhqMBiJ1pfJKYkPaIKPdSzCbKZKeE0TRjHydu2bZFSbxVtZE3j/X4Y0Pe9t5zWGWMQlLyjWMO5z/EsbKAsCpzjI6LDHof9DvHpiNMxokO1b3bftdg9ffGx/e4JcXzCadlDMFp5fPswVoszCEv9mEmoFg29KuuDX5Nlgwvon+c+Nvv5srcT8H8aVRc+15U19CCk+Iw7EvCMf4/2O+4ii01s8fFksab57SH494S/Phu83hr8+aBxyAOhpfBX4Zh5Q4w4DzwAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Recap: RNN-LM\",\n    \"title\": \"Recap: RNN-LM\",\n    \"src\": \"/cs224n-tensorflow/static/8e62c1e1ce531db0d16f543189fdd7db/e3189/15_RNN.png\",\n    \"srcSet\": [\"/cs224n-tensorflow/static/8e62c1e1ce531db0d16f543189fdd7db/a2ead/15_RNN.png 259w\", \"/cs224n-tensorflow/static/8e62c1e1ce531db0d16f543189fdd7db/6b9fd/15_RNN.png 518w\", \"/cs224n-tensorflow/static/8e62c1e1ce531db0d16f543189fdd7db/e3189/15_RNN.png 1035w\", \"/cs224n-tensorflow/static/8e62c1e1ce531db0d16f543189fdd7db/aa440/15_RNN.png 1500w\"],\n    \"sizes\": \"(max-width: 1035px) 100vw, 1035px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"h3\", null, \"Recap: decoding algorithms\"), mdx(\"p\", null, \"Question: Once you\\u2019ve trained your (conditional) language model, how do you use it to generate text?\\nAnswer: A decoding algorithm is an algorithm you use to generate text from your language model.\"), mdx(\"p\", null, \"We\\u2019ve learnt about two decoding algorithms: \"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Greedy decoding\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Beam search\")), mdx(\"h3\", null, \"One thing not discussed earlier about Beam Search:\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1035px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/cs224n-tensorflow/static/2ef81f4612d26bcdc203d43b7482b3ca/aa440/15_beam_size_k.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"74.90347490347492%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABcSAAAXEgFnn9JSAAACk0lEQVQ4y21U2ZKbMBD0//9UHpLKS7Z2fXIfxmDMjfGJBajTwvauNxVVtTUaxq1Rt2DS9RJpI1Gdgfossb9IlCeJ4ihxuDJ/umN/vkPFKv9cN6xXQ8r7PGk7YBoCeiwRFICdAQ6hxcCmAnzm5hGw2N6h7QA3v8Mh0sNIhwcfJugFpBA4CHDHFoemRFllaJoCeZ6gKNMxl5cZsiJFxWcqX3JdVTmG7va9Q8gBp3bAblMj1WzsVhqi6Qw700K0WCAmdpaNcDZDRGzVc2Lz/oHEtiG67jvho1PgcoAseNbrFTjwHJcLcKawxyPQXu+xgsqzRrJGqviF7N5hJ3C9CKQnHrkFojhGkuVIknREsN4g5RxvdyPUOgoj9A+OJ9nLkSVaIeFFJVxvg+V8ATfwoBc+rDKEngdw6xh6GYzxNLVhcHZchxvEn2RfhBwD4y0fzmYf8H0fju8hCH00awe1raPxbexdC7VronIMNBsfx32Nizr6g+yJibJcte+tQ5imgfV6DYtiO66HNNhA0JBW0yG4FrYD4floaUoXRfjfmDxbFRTYc13M5nMkloWM7mbLJSpDR22aKOlw/ucPio93VCq/WiFgnUHYbCAMQ/R9/0U4DAM19KDpOgzCMgyYmgaDMBlrJDFIsuKGKnZJkuxoFE3M8xwdr8945Cdh37Zw2Ykimb69YTqdUtMZpsSKRAZJVfdLxhprPOqcpim139KcLS95+WXK6BQ7vGUZLtTwpnZNEiT8Q0StiqL4x005GtnxR30Lun74vEaTV8ufY1BXiR0r3G63EVc6qtaCr6laD4KXWpz52p4f8wmSr/FEFajC18tZ1zWW1ElXeiotlRSEipVxtuPhsH5DH/6GCH4RPyH8HxhOKf4Cnj1tTnko2BoAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Effect of changing the Beam size K\",\n    \"title\": \"Effect of changing the Beam size K\",\n    \"src\": \"/cs224n-tensorflow/static/2ef81f4612d26bcdc203d43b7482b3ca/e3189/15_beam_size_k.png\",\n    \"srcSet\": [\"/cs224n-tensorflow/static/2ef81f4612d26bcdc203d43b7482b3ca/a2ead/15_beam_size_k.png 259w\", \"/cs224n-tensorflow/static/2ef81f4612d26bcdc203d43b7482b3ca/6b9fd/15_beam_size_k.png 518w\", \"/cs224n-tensorflow/static/2ef81f4612d26bcdc203d43b7482b3ca/e3189/15_beam_size_k.png 1035w\", \"/cs224n-tensorflow/static/2ef81f4612d26bcdc203d43b7482b3ca/aa440/15_beam_size_k.png 1500w\"],\n    \"sizes\": \"(max-width: 1035px) 100vw, 1035px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"h3\", null, \"Another family of decoding algorithms: Sampling Based Decoding\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Pure sampling\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Top-n sampling\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Both of these are more efficient than Beam serach, no more multiple hypotheses\")), mdx(\"h3\", null, \"Softmax temperature\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Softmax temperature is not a decoding algorithm.\"), \"\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"It\\u2019s a technique you can apply at test time, in conjunction with a decoding algorithm (such as beam search or sampling)\")), mdx(\"h3\", null, \"Decoding algorithms: in summary\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Greedy decoding\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Beam search\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Sampling methods\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Softmax temperature\")), mdx(\"h2\", null, \"2. NLG tasks and neural approaches to them\"), mdx(\"h3\", null, \"2.1.1. Summarization: task definition\"), mdx(\"p\", null, \"Task: given input text x, write a summary y which is shorter and contains the main information of x.\\nSummarization can be \"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Single-document \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Multi-document\")), mdx(\"p\", null, \"Training data sources:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Gigaword\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"LCSTS (Chinese microblogging)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"NYT, CNN/DailyMail\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Wikihow\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Sentence simplification:\"), \" is a different but related task: rewrite the source text in a simpler (sometimes shorter) way\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Simple Wikipedia: standard Wikipedia sentence \\u2192 simple version\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Newsela: news article \\u2192 version written for children\")), mdx(\"h3\", null, \"2.1.2. Summarization: How to do it\"), mdx(\"p\", null, \"Two main strategies:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Extractive summarization:\"), \" Select parts (typically sentences) of the original text to form a summary.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Abstractive summarization:\"), \" Generate new text using natural language generation techniques.\")), mdx(\"h4\", null, \"Explanation about pre-neural way of summarization:\"), mdx(\"p\", null, \"They were mostly extractive.\\nLike pre-neural MT, they typically had a pipeline:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Content selection:\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Sentence scoring functions: can be based on:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Presence of topic keywords,computed via e.g.tf-idf\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Features such as where the sentence appears in the document\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Graph-based algorithms: view the document as a set of sentences (nodes), with edges between each sentence pair\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Edge weight is proportional to sentence similarity\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Use graph algorithms to identify sentences which are central in the graph\"))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Information ordering\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Sentence realization\"))), mdx(\"h3\", null, \"2.1.3. How to evaluate Summarization:\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"ROUGE:\"), \" Recall-Oriented Understudy for Gisting Evaluation\\n\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"There is now a convenient Python implementation of ROUGE!\")), mdx(\"h3\", null, \"2.1.4. Neural summarization (2015 - present)\"), mdx(\"p\", null, \"2015: Rush et al. publish the first seq2seq summarization paper\\nSince 2015, there have been lots more developments!\\nCopy mechanisms use attention to enable a seq2seq system to easily copy words and phrases from the input to the output\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Clearly this is very useful for summarization\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Allowing both copying and generating gives us a \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"hybrid extractive/abstractive approach\"), \"\\nThere are several papers proposing copy mechanism variants:\")), mdx(\"p\", null, \"Big problem with copying mechanisms: \"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"They copy too much!\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Mostly long phrases,sometimes even whole sentences\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"What should be an abstractive system collapses to a mostly extractive system.\\nAnother problem:\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"They\\u2019re bad at overall content selection, especially if the input document is long\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"No overall strategy for selecting content\")), mdx(\"h3\", null, \"2.1.5. One solution: bottom-up summarization:\"), mdx(\"p\", null, \"Simple but effective!\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Content selection stage: Use a neural sequence-tagging model to tag words as include or don\\u2019t-include\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Bottom-up attention stage: The seq2seq+attention system can\\u2019t attend to words tagged don\\u2019t-include (apply a mask)\")), mdx(\"p\", null, \"In 2017 Paulus et al published a \\u201Cdeep reinforced\\u201D summarization model\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A Deep Reinforced Model for Abstractive Summarization, Paulus et al, 2017 \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1705.04304.pdf\"\n  }), \"https://arxiv.org/pdf/1705.04304.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Blog post: \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/\"\n  }), \"https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/\"))), mdx(\"h1\", null, \"This beginning to look like a paper tour\"), mdx(\"h3\", null, \"2.2.1. Dialogue\"), mdx(\"p\", null, \"\\u201CDialogue\\u201D encompasses a large variety of settings:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Task-oriented dialogue\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Assistive (e.g. customer service, giving recommendations, question answering, helping user accomplish a task like buying or booking something)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Co-operative (two agents solve a task together through dialogue)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Adversarial (two agents compete in a task through dialogue)\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Social dialogue\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Chit-chat (for fun or company)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Therapy / mental wellbeing\")))), mdx(\"h3\", null, \"2.2.2. Seq2seq-based dialogue\"), mdx(\"p\", null, \"\\u2022 However, it quickly became apparent that a na\\xEFve application of standard seq2seq+attention methods has serious pervasive deficiencies for (chitchat) dialogue:\\n\\u2022 Genericness / boring responses\\n\\u2022 Irrelevant responses (not sufficiently related to context) \\u2022 Repetition\\n\\u2022 Lack of context (not remembering conversation history) \\u2022 Lack of consistent persona\"), mdx(\"h3\", null, \"2.2.3. Negotiation dialogue\"), mdx(\"p\", null, \"Papers:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A Neural Conversational Model, Vinyals et al, 2015 \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1506.05869.pdf\"\n  }), \"https://arxiv.org/pdf/1506.05869.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Neural Responding Machine for Short-Text Conversation, Shang et al, 2015 \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://www.aclweb.org/anthology/P15-1152\"\n  }), \"https://www.aclweb.org/anthology/P15-1152\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A Diversity-Promoting Objective Function for Neural Conversation Models, Li et al, 2016 \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1510.03055.pdf\"\n  }), \"https://arxiv.org/pdf/1510.03055.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Why are Sequence-to-Sequence Models So Dull?, Jiang et al, 2018 \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/jiang-why-2018.pdf\"\n  }), \"https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/jiang-why-2018.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A Persona-Based Neural Conversation Model, Li et al 2016, \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1603.06155.pdf\"\n  }), \"https://arxiv.org/pdf/1603.06155.pdf\"), \" \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Personalizing Dialogue Agents: I have a dog, do you have pets too?, Zhang et al, 2018 \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1801.07243.pdf\"\n  }), \"https://arxiv.org/pdf/1801.07243.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Deal or No Deal? End-to-End Learning for Negotiation Dialogues, Lewis et al, 2017 \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1706.05125.pdf\"\n  }), \"https://arxiv.org/pdf/1706.05125.pdf\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Hierarchical Text Generation and Planning for Strategic Dialogue, Yarats et al, 2018 \", mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1712.05846.pdf\"\n  }), \"https://arxiv.org/pdf/1712.05846.pdf\"))), mdx(\"h3\", null, \"2.3.1. Storytelling\"), mdx(\"p\", null, \"Most neural storytelling work uses some kind of prompt\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Generate a story-like paragraph given an image\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Generate a story given a brief writing prompt\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Generate the next sentence of a story, given the story so far (story continuation)\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This is different to the previous two, because we are not concerned with the system\\u2019s performance over several generated sentences\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Neural storytelling is taking off!\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The first Storytelling Workshop was held in 2018\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It held a competition (generate a story to accompany a sequence of 5 images)\")))), mdx(\"h3\", null, \"2.4.1. Poetry generation: Hafez\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Hafez:\"), \" a poetry generation system by Ghazvininejad et al\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Main idea:\"), \" Use a Finite State Acceptor (FSA) to define all possible sequences that obey the desired rhythm constraints. Then use the FSA to constrain the output of a RNN-LM.\"), mdx(\"h3\", null, \"2.5.1. Non-autoregressive generation for NMT\"), mdx(\"h1\", null, \"Thoughts about this lecture\"), mdx(\"p\", null, \"Actually This lecture does not provide a lot of practival things that can be utilized directly in your day job.\\nFor this reasons, this writeup will be fairly superfecial and not comprehensive.\"), mdx(\"p\", null, \"However it does provide very useful NLG task decription, current approaches and thier problems and papers/resources relevant these tasks. This writeup just aims to bring all this knowledge together in one place.\\nThis is intented to serve as a starting point to anyone working on these problems.\"), mdx(\"h2\", null, \"3. NLG evaluation\"), mdx(\"p\", null, \"How do we evaluate and compare the performance one NLG system to another?\"), mdx(\"p\", null, \"Word overlap based metrics (BLEU, ROUGE, METEOR, F1, etc.) are not ideal for machine translation.\"), mdx(\"p\", null, \"What about perplexity?\\nWord embedding based metrics?\"), mdx(\"p\", null, \"We have no automatic metrics to adequately capture overall quality (i.e. a proxy for human quality judgment).\\nBut we can define more focused automatic metrics to capture particular aspects of generated text:\\n\\u2022 Fluency (compute probability w.r.t. well-trained LM)\\n\\u2022 Correct style (prob w.r.t. LM trained on target corpus)\\n\\u2022 Diversity (rare word usage, uniqueness of n-grams)\\n\\u2022 Relevance to input (semantic similarity measures)\\n\\u2022 Simple things like length and repetition\\n\\u2022 Task-specific metrics e.g. compression rate for summarization\\nThough these don\\u2019t measure overall quality, they can help us track some important qualities that we care about.\"), mdx(\"p\", null, \"Human evaluation\\n\\u2022 Human judgments are regarded as the gold standard\\n\\u2022 Of course, we know that human eval is slow and expensive\\n\\u2022 ...but are those the only problems?\\n\\u2022 Supposing you do have access to human evaluation:\\nDoes human evaluation solve all of your problems?\\n\\u2022 No!\"), mdx(\"h3\", null, \"3.x.x Detailed human eval of controllable chatbots\"), mdx(\"p\", null, \"Ultimately, we designed a detailed human evaluation system that separates out the important factors that contribute to overall chatbot quality:\\nWhat makes a good conversation? How controllable attributes affect human judgments, See et al, 2019 \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1902.08654.pdf\"\n  }), \"https://arxiv.org/pdf/1902.08654.pdf\")), mdx(\"h3\", null, \"Possible new avenues for NLG eval?\"), mdx(\"h2\", null, \"4. Thoughts on NLG research, current trends, and the future\"), mdx(\"h3\", null, \"4.1. Exciting current trends in NLG\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Incorporating discrete latent variables into NLG\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"May help with modeling structure in tasks that really need it, like storytelling, task-oriented dialogue, etc\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Alternatives to strict left-to-right generation\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Parallel generation, iterative refinement, top-down generation for longer pieces of text\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Alternative to maximum likelihood training with teacher forcing\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"More holistic sentence-level (rather than word-level)\\nobjectives\")))), mdx(\"p\", null, \"Neural NLG community is rapidly maturing\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"During the early years of NLP + Deep Learning, community was mostly transferring successful NMT methods to NLG tasks.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Now, increasingly more inventive NLG techniques emerging, specific to non-NMT generation settings.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Increasingly more (neural) NLG workshops and competitions, especially focusing on open-ended NLG:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"NeuralGen workshop\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Storytelling workshop\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Alexa challenge\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"ConvAI2 NeurIPS challenge\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"These are particularly useful to organize the community, increase reproducibility, standardize eval, etc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The biggest roadblock for progress is eval\")), mdx(\"h3\", null, \"8 things I\\u2019ve learnt from working in NLG\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"The more open-ended the task, the harder everything becomes. \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"Constraints are sometimes welcome!\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Aiming for a specific improvement can be more manageable than aiming to improve overall generation quality.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"If you\\u2019re using a LM for NLG: improving the LM (i.e. perplexity) will most likely improve generation quality. \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"...but it's not the only way to improve generation quality.\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Look at your output, a lot\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"You need an automatic metric, even if it's imperfect. \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"You probably need several automatic metrics.\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"If you do human eval, make the questions as focused as possible.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Reproducibility is a huge problem in today's NLP + Deep Learning, and a huger problem in NLG. \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"Please, publicly release all your generated output along with your paper!\")), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Working in NLG can be very frustrating. But also very funny...\")));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"items":[{"url":"#plan","title":"Plan:","items":[{"url":"#what-is-nlg","title":"What is NLG:"}]},{"url":"#1-recap-what-we-already-know-about-nlg","title":"1. Recap what we already know about NLG","items":[{"url":"#recap-training-a-conditional-rnn-lm","title":"Recap: training a (conditional) RNN-LM:"},{"url":"#recap-decoding-algorithms","title":"Recap: decoding algorithms"},{"url":"#one-thing-not-discussed-earlier-about-beam-search","title":"One thing not discussed earlier about Beam Search:"},{"url":"#another-family-of-decoding-algorithms-sampling-based-decoding","title":"Another family of decoding algorithms: Sampling Based Decoding"},{"url":"#softmax-temperature","title":"Softmax temperature"},{"url":"#decoding-algorithms-in-summary","title":"Decoding algorithms: in summary"}]},{"url":"#2-nlg-tasks-and-neural-approaches-to-them","title":"2. NLG tasks and neural approaches to them","items":[{"url":"#211-summarization-task-definition","title":"2.1.1. Summarization: task definition"},{"url":"#212-summarization-how-to-do-it","title":"2.1.2. Summarization: How to do it","items":[{"url":"#explanation-about-pre-neural-way-of-summarization","title":"Explanation about pre-neural way of summarization:"}]},{"url":"#213-how-to-evaluate-summarization","title":"2.1.3. How to evaluate Summarization:"},{"url":"#214-neural-summarization-2015---present","title":"2.1.4. Neural summarization (2015 - present)"},{"url":"#215-one-solution-bottom-up-summarization","title":"2.1.5. One solution: bottom-up summarization:"}]}]},{"url":"#this-beginning-to-look-like-a-paper-tour","title":"This beginning to look like a paper tour","items":[{"items":[{"url":"#221-dialogue","title":"2.2.1. Dialogue"},{"url":"#222-seq2seq-based-dialogue","title":"2.2.2. Seq2seq-based dialogue"},{"url":"#223-negotiation-dialogue","title":"2.2.3. Negotiation dialogue"},{"url":"#231-storytelling","title":"2.3.1. Storytelling"},{"url":"#241-poetry-generation-hafez","title":"2.4.1. Poetry generation: Hafez"},{"url":"#251-non-autoregressive-generation-for-nmt","title":"2.5.1. Non-autoregressive generation for NMT"}]}]},{"url":"#thoughts-about-this-lecture","title":"Thoughts about this lecture","items":[{"url":"#3-nlg-evaluation","title":"3. NLG evaluation","items":[{"url":"#3xx-detailed-human-eval-of-controllable-chatbots","title":"3.x.x Detailed human eval of controllable chatbots"},{"url":"#possible-new-avenues-for-nlg-eval","title":"Possible new avenues for NLG eval?"}]},{"url":"#4-thoughts-on-nlg-research-current-trends-and-the-future","title":"4. Thoughts on NLG research, current trends, and the future","items":[{"url":"#41-exciting-current-trends-in-nlg","title":"4.1. Exciting current trends in NLG"},{"url":"#8-things-ive-learnt-from-working-in-nlg","title":"8 things I’ve learnt from working in NLG"}]}]}]},"parent":{"__typename":"File","relativePath":"15_natural_language_generation.md"},"frontmatter":{"metaTitle":"This is the title tag of this page 15","metaDescription":"This is the meta description"}},"allMdx":{"edges":[{"node":{"fields":{"slug":"/01_word_vectors","title":"Word Vectors"}}},{"node":{"fields":{"slug":"/03_word_window","title":"03. Word Window Classification, Neural Networks, and Matrix Calculus"}}},{"node":{"fields":{"slug":"/00_toc","title":"Table of Contents"}}},{"node":{"fields":{"slug":"/07_vanishing_gradients_fancy_rnn","title":"07. Vanishing Gradients and Fancy RNNs"}}},{"node":{"fields":{"slug":"/05_linguistic_structure_dependency_parsing","title":"05. Linguistic Structure: Dependency Parsing"}}},{"node":{"fields":{"slug":"/codeblock","title":"Syntax Highlighting"}}},{"node":{"fields":{"slug":"/15_natural_language_generation","title":"15. Natural Language Generation"}}},{"node":{"fields":{"slug":"/06_language_models_rnn","title":"06. The probability of a sentence? Recurrent Neural Networks and Language Models"}}},{"node":{"fields":{"slug":"/","title":"Welcome"}}},{"node":{"fields":{"slug":"/extra_stuff","title":"Extra Stuff"}}},{"node":{"fields":{"slug":"/codeblock/1-index","title":"Sub Page"}}},{"node":{"fields":{"slug":"/01_word_vectors/01_count_based","title":"Count Based Word Vectors"}}},{"node":{"fields":{"slug":"/01_word_vectors/02_assignment1","title":"Assignment 1"}}},{"node":{"fields":{"slug":"/01_word_vectors/03_word2vec","title":"Word2Vec"}}},{"node":{"fields":{"slug":"/01_word_vectors/singular_value_decomposition","title":"Singular Value Decomposition"}}}]}},"pageContext":{"id":"f669c146-88cb-5ecf-a26c-f781f248682c"}}}