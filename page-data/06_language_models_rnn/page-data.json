{"componentChunkName":"component---src-templates-docs-js","path":"/06_language_models_rnn","result":{"data":{"site":{"siteMetadata":{"title":"CS224n Natural Language Processing Tutorial | Abhishek Dutt","docsLocation":"https://abhishekdutt.github.io/cs224n-tensorflow/"}},"mdx":{"fields":{"id":"5f99498f-8c16-57b1-9951-c2989bc4055d","title":"06. The probability of a sentence? Recurrent Neural Networks and Language Models","slug":"/06_language_models_rnn"},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"06. The probability of a sentence? Recurrent Neural Networks and Language Models\",\n  \"metaTitle\": \"This is the title tag of this page\",\n  \"metaDescription\": \"This is the meta description\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Language Modeling is the task of predicting what word comes next.\"), mdx(\"p\", null, \"Models that assign probabilities to sequences of words are called language models or LMs. \"), mdx(\"p\", null, \"Language models compute the probability of occurrence of a number of words in a particular sequence. \"), mdx(\"p\", null, \"We\\u2019ll see how to use n-gram models to estimate the probability of the last word of an n-gram given the previous words, and also to assign probabilities to entire sequences. \"), mdx(\"p\", null, \"In later chapters we\\u2019ll introduce more sophisticated language models like the RNN LMs of Chapter 9.\"), mdx(\"p\", null, \"N-Grams Language Model:\"), mdx(\"p\", null, \"Let\\u2019s begin with the task of computing P(w|h), the probability of a word w given some history h.\\nSuppose the history h is \\u201Cits water is so transparent that\\u201D and we want to know the probability that the next word is \\\"the\\\":\"), mdx(\"p\", null, \"P(the|its water is so transparent that). (3.1)\"), mdx(\"p\", null, \"One way to estimate this probability is from relative frequency counts:\\nTake a very large corpus, count the number of times we see \\\"its water is so transparent that\\\", and count the number of times this is followed by \\\"the\\\". This would be answering the question \\u201COut of the times we saw the history h, how many times was it followed by the word w\\u201D, as follows:\"), mdx(\"p\", null, \"P(the|its water is so transparent that) = C(its water is so transparent that the) / C(its water is so transparent that) (3.2)\"), mdx(\"p\", null, \"While this method of estimating probabilities directly from counts works fine in many cases, it turns out that even the web isn\\u2019t big enough to give us good estimates in most cases. \"), mdx(\"p\", null, \"Howerver there are two issues with this approach: Either the neumerator or the denominator do not exists in the corpus.\"), mdx(\"p\", null, \"This is because language is creative; new sentences are created all the time, and we won\\u2019t always be able to count entire sentences. Even simple extensions of the example sentence may have counts of zero on the web.\"), mdx(\"p\", null, \"Secondly:\\nSimilarly, if we wanted to know the joint probability of an entire sequence of words like \\\"its water is so transparent\\\", we could do it by asking \\u201Cout of all possible sequences of five words, how many of them are \\\"its water is so transparent?\\\"\\u201D. We would have to get the count of \\\"its water is so transparent\\\" and divide by the sum of the counts of all possible five word sequences. That seems rather a lot to estimate!\"), mdx(\"p\", null, \"Now how can we compute probabilities of entire sequences like P(w1 , w2 , ..., wn )? One thing we can do is decompose this probability using the chain rule of proba- bility:\"), mdx(\"p\", null, \"Applying the chain rule to words, we get\"), mdx(\"p\", null, \"P(wn) = P(w1)P(w2|w1)P(w3|w2,w1)...P(wn|wn\\u22121,wn-2,...,w1)\"), mdx(\"p\", null, \"  n\\n= \\u220F P(wk|wk\\u22121,...,w1) (3.4)\\nk=1\"), mdx(\"p\", null, \"But we still dont know how to calculate P(wk|wk-1,...w1) for all k=1..n.\\nAs we said above, we can\\u2019t just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before!\"), mdx(\"p\", null, \"The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words.\"), mdx(\"p\", null, \"The bigram model, for example, approximates the probability of a word given all the previous words P(wn|wn\\u22121) by using only the conditional probability of the preceding word P(wn|wn\\u22121). In other words, instead of computing the probability\"), mdx(\"p\", null, \"P(the|Walden Pond\\u2019s water is so transparent that) (3.5)\\nwe approximate it with the probability\\nP(the|that) (3.6)\"), mdx(\"p\", null, \"When we use a bigram model to predict the conditional probability of the next word, we are thus making the following approximation:\\nP(wn|wn\\u22121) \\u2248 P(wn|wn\\u22121)\"), mdx(\"p\", null, \"Thus, the general equation for this n-gram approximation to the conditional probability of the next word in a sequence is\\nP(wn|wn\\u22121_1) \\u2248 P(wn|wn\\u22121_n\\u2212N+1 ) (3.8) \"), mdx(\"p\", null, \"Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:\"), mdx(\"p\", null, \"P(wn1) \\u2248 \\u220F P(wk|wk\\u22121)  (3.9)\"), mdx(\"p\", null, \"How do we estimate these bigram or n-gram probabilities? An intuitive way to estimate probabilities is called maximum likelihood estimation or MLE. We get the MLE estimate for the parameters of an n-gram model by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1.\"), mdx(\"p\", null, \"For example, to compute a particular bigram probability of a word y given a previous word x, we\\u2019ll compute the count of the bigram C(xy) and normalize by the\\nsum of all the bigrams that share the same first word x:\\nP(wn|wn\\u22121) = C(wn\\u22121,wn) / \\uDBFF\\uDC01w C(wn\\u22121,w) (3.10)\"), mdx(\"p\", null, \"We can simplify this equation, since the sum of all bigram counts that start with a given word wn\\u22121 must be equal to the unigram count for that word wn\\u22121 (the reader should take a moment to be convinced of this):\\nP(wn|wn\\u22121) = C(wn\\u22121,wn) / C(wn\\u22121) (3.11) \"), mdx(\"hr\", null), mdx(\"p\", null, \"n-gram LM more succintly\\nThe probability of a sequence of m words {w1, ..., wm} is denoted as P(w1, ..., wm). Since the number of words coming before a word, wi, varies depending on its location in the input document, P(w1, ..., wm) is usually conditioned on a window of n previous words rather than all previous words:\"), mdx(\"p\", null, \"P(w1, ..., wm) = \\u220F P(wi|w1, ..., wi\\u22121) \\u2248 \\u220F P(wi|wi\\u2212n, ..., wi\\u22121) (1)\"), mdx(\"p\", null, \"Uses: Equation 1 is especially useful for speech and translation systems when determining whether a word sequence is an accurate transla- tion of an input sentence. In existing language translation systems, for each phrase / sentence translation, the software generates a num- ber of alternative word sequences (e.g. {I have, I had, I has, me have, me had) and scores them to identify the most likely translation sequence.\"), mdx(\"p\", null, \"1.2 n-gram Language Models\\nTo compute the probabilities mentioned above, the count of each n-gram could be compared against the frequency of each word. This is called an n-gram Language Model. \"), mdx(\"p\", null, \"For instance, if the model takes bi-grams, the frequency of each bi-gram, calculated via combining a word with its previous word, would be divided by the frequency of the corresponding uni-gram. Equations 2 and 3 show this relationship for bigram and trigram models.\\np(w2|w1) = count(w1, w2)/count(w1) (2)\\np(w3|w1, w2) = count(w1, w2, w3)/count(w1, w2) (3) \"), mdx(\"p\", null, \"But how long should the context be? In some cases, the window of past consecutive n words may not be suf- ficient to capture the context. For instance, consider the sentence \\\"As the proctor started the clock, the students opened their \\\". If the window only conditions on the previous three words \\\"the students opened their\\\", the probabilities calculated based on the corpus may suggest that the next word be \\\"books\\\" - however, if n had been large enough to include the \\\"proctor\\\" context, the probability might have suggested \\\"exam\\\".\"), mdx(\"p\", null, \"This leads us to two main issues with n-gram Language Models: Sparsity and Storage.\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Sparsity problems with n-gram Language models\\nSparsity problems with these models arise due to two issues.\\nFirstly, note the numerator of Equation 3. If w1, w2 and w3 never appear together in the corpus, the probability of w3 is 0. To solve this, a small \\u03B4 could be added to the count for each word in the vocabulary. This is called smoothing.\\nSecondly, consider the denominator of Equation 3. If w1 and w2 never occurred together in the corpus, then no probability can be calculated for w3. To solve this, we could condition on w2 alone. This is called backoff.\\nIncreasing n makes sparsity problems worse. Typically, n \\u2264 5.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Storage problems with n-gram Language models\\nWe know that we need to store the count for all n-grams we saw in the corpus. As n increases (or the corpus size increases), the model size increases as well.\")), mdx(\"p\", null, \"Window-based Neural Language Model:\\nThe \\\"curse of dimensionality\\\" above was first tackled by Bengio et al in A Neural Probabilistic Language Model, which introduced the first large-scale deep learning for natural language processing model.\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1035px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/cs224n-tensorflow/static/e0c9396883cecad32ada158107b6525e/aa440/06_fixed_window_based_neural_lm.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"74.90347490347492%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABcSAAAXEgFnn9JSAAAChUlEQVQ4y31T2XLiMBDM///C/sfu24achAQSJ7BAuGICLDY+seRDlqXO2A4mbKpWVW2Nx1JPz+GzQmk4TMPjGmGiEcQaLr2zTGOflEDl/2q7vL7D6Uy5tNaEysSZkMDMAgareh/+BQyyTa+2/2w05jvgcanRNTWMd2BE/p4JbMOKriJrCOkVsebIkCAlaJVBS7LjCFCiPgX9CSAVMZTOyMoJ8pNIVyorwvIx3MxxNzZwO+ih033A8/ML1psNHNdDoRQRELeqL+y4RTbgRqxRdSBrCH3Hg9F7wmLxBstxYNs2dgTLspBlWXVQaVXthnWOu/VP/Jr9wHDXqQIe66hxpsjBOQfjjPKhqpP97zocLrOeOiP0VheEFuEaUcJOCcvHbDpFp9PBY6uF6dUVgvEYwWJB/ElduS8pHZYU34M2KW+3W7TbbRj9PvqjEYaDAXr393Bd92Qsgv4IVusK2wvC5TWsyxv4T30oWdRtKxWWFyLGMJ5M4HgehBCISVkYhk39GlVBCElBYhLgzOeQnovc95sJaAgDasTg4QEDUmV0u5Xa89/nuLm5pQZZiGmArSBHRLufUoeZxNvWh0cljyimHUqwpDimHAcBlv0X7CZjsMCD63vwCPZuh2gfYOUItIcBTCtFbxLidZVguk7Qfd1XvvYwxNJOj4QpE9i7HKHFIQIJSRFzP4fiNIMJjUVB0LJOS8nPLpSK1ImvSbkguSWYyRCSgsiMqNMBYpv+ilxBcEFq7aq+M6qd5/nUMA+muUQuJVbrNfI8PyrMeQ6+oVlccdpjsHdOpAxsyUixQJ7S95hXCspmCbpcNiyKIpRzfNgrwpI5jVMUaQGZyG8osuLk1/rfKs99ADa1fDuPkKTRAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Fixed-Window based nerual Language Model\",\n    \"title\": \"Fixed-Window based nerual Language Model\",\n    \"src\": \"/cs224n-tensorflow/static/e0c9396883cecad32ada158107b6525e/e3189/06_fixed_window_based_neural_lm.png\",\n    \"srcSet\": [\"/cs224n-tensorflow/static/e0c9396883cecad32ada158107b6525e/a2ead/06_fixed_window_based_neural_lm.png 259w\", \"/cs224n-tensorflow/static/e0c9396883cecad32ada158107b6525e/6b9fd/06_fixed_window_based_neural_lm.png 518w\", \"/cs224n-tensorflow/static/e0c9396883cecad32ada158107b6525e/e3189/06_fixed_window_based_neural_lm.png 1035w\", \"/cs224n-tensorflow/static/e0c9396883cecad32ada158107b6525e/aa440/06_fixed_window_based_neural_lm.png 1500w\"],\n    \"sizes\": \"(max-width: 1035px) 100vw, 1035px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"p\", null, \"This brings us to the main course of this lecture:\\nRecurrent Neural Networks (RNN):\"), mdx(\"p\", null, \"RNN Advantages:\\n\\u2022 Can process any length input\\n\\u2022 Computation for step t can (in theory) use information from many steps back\\n\\u2022 Model size doesn\\u2019t increase for longer input\\n\\u2022 Same weights applied on every timestep, so there is symmetry in how inputs are processed.\"), mdx(\"p\", null, \"RNN Disadvantages:\\n\\u2022 Recurrent computation is slow\\n\\u2022 In practice, difficult to access information from many steps back\"), mdx(\"p\", null, \"2.2 Advantages, Disadvantages and Applications of RNNs RNNs have several advantages:\\nThey can process input sequences of any length\\nThe model size does not increase for longer input sequence lengths\\nComputation for step t can (in theory) use information from many steps back.\\nThe same weights are applied to every timestep of the input, so there is symmetry in how inputs are processed\\nHowever, they also have some disadvantages:\\nComputation is slow - because it is sequential, it cannot be paral- lelized\\nIn practice, it is difficult to access information from many steps back due to problems like vanishing and exploding gradients, which we discuss in the following subsection\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1035px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/cs224n-tensorflow/static/d313d3165eb3ceb86c30c70449904685/aa440/06_rnn_lm.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"74.90347490347492%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABcSAAAXEgFnn9JSAAACnklEQVQ4y31UTXPTMBDN/7/xC7hw5coMw6EdmJYCbaFlptCkTuPUjr9j2bJkybb0WClJ24EOO7Pe2JKedt++zWwyFnlrkTCLWlgwadFIkFuUfOedsgCcA+M4QggB3nXYshqcc0h6b5oGUkrMxgmYp8DPCFjmwGUInK8sRYubjcUFxbjeAVoKlh5mmqAHjZxtIXuFgjEMw4iJvs+YbCmzCltR+Eg5YGdPWfm3PdhuZRfv2RXk2OKBBY97Z23HUdENWVkiKwq0vEPTthioNGIDhh7GWg92cGea1t8v3uDd/DXe/nqFb9Gxx5z5Tcb4Mpy7tEfyw8Hn9vzbZCYILaEnhZrXKJrSr8/wH+NNC0uZuEb0LfcHOvrmTHbCoWKiJtRFBUu/fckiLdCtY7DVA8z+8KA1JgcYrrE5/YKyrpEef0Iax8gvfiC9XSCf3yG9vEJBVOUnZzB0xgPW2y3yNMV9EKAgDpMkQUscaipbhA/IPn+FUArp0UdwxzVdUC9XKH/PUZ5/Ryckkg9HT4CGuBi0gtY9eim8lg5cjbR5YLsSs2Dps2/jBHWWw/Y9EsrSWXo7R097PaAaDGVg0PUGXE4vNsBp7Pq+gZA97hKBpOqoCQqLmNQwDLhekajVsAPMK45gXeIupHKrnng2j0o7yETQ7Sc3HZWncLlokG4HBBHDPB58Rac3TmZ7Hb7YXot/dBcmjDRpkJSSRo6o0RbhpvZrSwJXep9hRnxEUYQ43iDekFMn9aj/wqdZTyI/bkGwQJalcM1czG89r+FqCUWN84BuuGuShRtuJ1bda/RlD1UpqJqaVWtIyqqKttBMo9xUUFxRqT06+oNwWbecP1Yyczcc0P0EqAld1IERV3zN0QQUQw6+4hDUkIENJBHzMlME+gd/o4KSsfPDHwAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"RNN Language Model\",\n    \"title\": \"RNN Language Model\",\n    \"src\": \"/cs224n-tensorflow/static/d313d3165eb3ceb86c30c70449904685/e3189/06_rnn_lm.png\",\n    \"srcSet\": [\"/cs224n-tensorflow/static/d313d3165eb3ceb86c30c70449904685/a2ead/06_rnn_lm.png 259w\", \"/cs224n-tensorflow/static/d313d3165eb3ceb86c30c70449904685/6b9fd/06_rnn_lm.png 518w\", \"/cs224n-tensorflow/static/d313d3165eb3ceb86c30c70449904685/e3189/06_rnn_lm.png 1035w\", \"/cs224n-tensorflow/static/d313d3165eb3ceb86c30c70449904685/aa440/06_rnn_lm.png 1500w\"],\n    \"sizes\": \"(max-width: 1035px) 100vw, 1035px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \"), \"\\nExplain dimensions here:\\nx1,...,xt\\u22121,xt,xt+1,...xT:thewordvectorscorrespondingtoacor- pus with T words.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"ht = \\u03C3(W(hh)ht\\u22121 + W(hx)xt): the relationship to compute the hidden layer output features at each time-step t\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"xt \\u2208 Rd: input word vector at time t.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Whx \\u2208 RDh\\xD7d: weights matrix used to condition the input word vector, xt\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Whh \\u2208 RDh\\xD7Dh: weights matrix used to condition the output of the previous time-step, ht\\u22121\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"ht\\u22121 \\u2208 RDh : output of the non-linear function at the previous time-step, t \\u2212 1. h0 \\u2208 RDh is an initialization vector for the hidden layer at time-step t = 0.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\u03C3(): the non-linearity function (sigmoid here)\")), mdx(\"p\", null, \"y\\u02C6 = so f tmax(W(S)h ): the output probability distribution over the vocabulary at each time-step t. Essentially, y\\u02C6 is the next predicted t\\nword given the document context score so far (i.e. ht\\u22121) and the (t) (S) |V|\\xD7D |V|\\nlastobservedwordvectorx .Here,W \\u2208R h andy\\u02C6\\u2208R where |V| is the vocabulary.\"), mdx(\"p\", null, \"How to Train RNNs\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1035px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/cs224n-tensorflow/static/4cd2340254554c3837dab137d588946d/aa440/06_training_rnn_ln.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"74.90347490347492%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABcSAAAXEgFnn9JSAAACPUlEQVQ4y4WSiVLjMBBE8///tgsFJHFCLsd24kM2tnwf0VtJIZAFqpjy1Hh0tLpbmo2TQpSKtFKUrSJvFOdckele6v+8VhTNdT6R1x6l9Pc9TczGCzghrHQ+eopNovh7VKxjxeIMf1wzDusIHo5wSPk/1Gc1oDNzWl8XVEUGjUZvJhh0rSe6QpKKkLaWyFxQZDF1mRNFEXES03YNF3WhbmtbLUMDGJ5OPD+/cFi5vL5s2K72+LsAZ7HCWa1YLJfM5wv+Pjzy8jJnt98RuAGd6GhFiwwkXdm/A+oYpwtRkrA77Jg71w3z5Zyls2S729re1P1hz2azsb3v+XgHz2ZXde+qtWRlqJpvUIztaCfLskRKSVM3DM1gx02tyoq3/I2qqiiKgjzPyYscWUo79uFhmUr8dUB8SEjchOPxyGKxwD24iKMg9VLEXrB/3bPU8tfrNU9PT/Z/u93iOA6u6zKO41VyVdcE5wDXcwmjkKZpCPyAMAw5nU+EcYh/8onj2DI7ac+FEDb7vmeapo9Ln93ej4m2ba3MWh+QiMTK6/rObgqCwEo0IMYOs+8GbOJyuXxK7jSQWWy8M16Y7OqOsRmZ2olSlpatmTcMrb9axddH/emhXuRq34wPURjh+R5SSIa3gT7ryURm3561QLPyPI80TT9A7utM8T3M6VVdEYuYNEvtLQ7DwPl8trZkWUbXdd8ArwzvmlvefDMsjFwDZgDMpRh/jex7wPt7mH314H7yt/hp7cycbmT8RP+3/InEP2I6gNt8S+lUAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"RNN Language Model\",\n    \"title\": \"RNN Language Model\",\n    \"src\": \"/cs224n-tensorflow/static/4cd2340254554c3837dab137d588946d/e3189/06_training_rnn_ln.png\",\n    \"srcSet\": [\"/cs224n-tensorflow/static/4cd2340254554c3837dab137d588946d/a2ead/06_training_rnn_ln.png 259w\", \"/cs224n-tensorflow/static/4cd2340254554c3837dab137d588946d/6b9fd/06_training_rnn_ln.png 518w\", \"/cs224n-tensorflow/static/4cd2340254554c3837dab137d588946d/e3189/06_training_rnn_ln.png 1035w\", \"/cs224n-tensorflow/static/4cd2340254554c3837dab137d588946d/aa440/06_training_rnn_ln.png 1500w\"],\n    \"sizes\": \"(max-width: 1035px) 100vw, 1035px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"p\", null, \"The amount of memory required to run a layer of RNN is pro- portional to the number of words in the corpus. We can consider a sentence as a minibatch, and a sentence with k words would have k word vectors to be stored in memory. Also, the RNN must maintain two pairs of W, b matrices. As aforementioned, while the size of W could be very large, it does not scale with the size of the corpus (un- like the traditional language models). For a RNN with 1000 recurrent layers, the matrix would be 1000 \\xD7 1000 regardless of the corpus size.\\nRNNs can be used for many tasks, such as tagging (e.g. part-of- speech, named entity recognition), sentence classification (e.g. sentiment classification), and encoder modules (e.g. questiton answering, machine translation, and many other tasks. In the latter two applica- tions, we want a representation for the sentence, which we can obtain by taking the element-wise max or mean of all hidden states of the timesteps in that sentence.\"), mdx(\"p\", null, \"Application: tagging, sentence classification, question anser, machine translation, speech recognization, summarization etc.\"), mdx(\"h1\", null, \"{TODO} {IMPORTANT} Backpropagation for RNNs\"), mdx(\"p\", null, \"[TODO]\", \" Explain Vanishing gradient, gradient explosion\"), mdx(\"h1\", null, \"Text Generation RNNs\"), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1035px\"\n    }\n  }), \"\\n      \", mdx(\"a\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/cs224n-tensorflow/static/930f16d5d816fe6bda6835fbdb1f8bf8/aa440/06_text_generation_rnn.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }), \"\\n    \", mdx(\"span\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"74.90347490347492%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABcSAAAXEgFnn9JSAAACYUlEQVQ4y32TaXPbOAyG/f//0+40naTNTrfbreMcjmNbsqTIuu+D1PEsJH+ws+OWMyBBgHiJlwAXXQ9xBUULaQ2ZSKlOEpXn1cshb0Ta09lJn+JC8akOGSPjCItGj/xjKP7etSwtzf2b5sFSPNqt6A2bo2J56Pm+71m/d9ytFV83mh9Gx9LU3Mo+zHtmSEFc6FZxNEx808I7WDimgf22x9pscbY7ERPLeOOwf8Xcb3h9eWS7eeZptWTzshJ5IInDM+CgB9pE0xUDeVDguT5N3JIJxyZpZz0II5x3F9txCKOIOE7wfJ9Q7H4QopQ+A3IxsiLDPtioRuFIcJEV6E6jG40SJo7lMHYj18YENo3FpIzDOBuGcaCWqvRVT1M1qFTN4GVQompZkxJd6Pns/+Wc4Xi+se97wjia9STLaFRDqxSpZC5hREk8r5cZjePHjBeDHEg9n7GR96prIqE8Aws91TSkfkCVpJKx+MR2jeYHwGlKV8/Y99/wTJP3Pz9zDAK8T7e4U/UfX3C/fedo2bh/3MzFmC78ZYbTlK83RI/PlFJB/8tftF3H8fMddZqSvG6JliuqNMO7uaNpWzrx/xawlwqOem53rL05P2vouFLlHC1UXfNw8r1t6YdzE1+lfGn045oHo6LTip+7gqJqMb0Kw6spRf8hNsbTF/tthifbiO0V/LurBbDldpUKoOLJyHh1WtK84u6pmIOG4WOrXIIuLg39AK4fz3qQVLTyFGXTEcT5bDu8h1y22VXKSvqsqqp5o7XGO7qUZcnD8ieJ9J1tWxj73ezPs2TuwvV6Ld8uvJrhf7YRgheBz8jsAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"a\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Generating Text RNN LM\",\n    \"title\": \"Generating Text RNN LM\",\n    \"src\": \"/cs224n-tensorflow/static/930f16d5d816fe6bda6835fbdb1f8bf8/e3189/06_text_generation_rnn.png\",\n    \"srcSet\": [\"/cs224n-tensorflow/static/930f16d5d816fe6bda6835fbdb1f8bf8/a2ead/06_text_generation_rnn.png 259w\", \"/cs224n-tensorflow/static/930f16d5d816fe6bda6835fbdb1f8bf8/6b9fd/06_text_generation_rnn.png 518w\", \"/cs224n-tensorflow/static/930f16d5d816fe6bda6835fbdb1f8bf8/e3189/06_text_generation_rnn.png 1035w\", \"/cs224n-tensorflow/static/930f16d5d816fe6bda6835fbdb1f8bf8/aa440/06_text_generation_rnn.png 1500w\"],\n    \"sizes\": \"(max-width: 1035px) 100vw, 1035px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n  \"), \"\\n    \")), mdx(\"h1\", null, \"Note:\"), mdx(\"p\", null, \"RNN discussed here are just \\\"vanilla\\\" RNNs and not used as it is.\\nMore advanced forms of RNNs such as LSTM, GRU, multi layer, bi-directional used in practice which are discussed in lecture 07.\"));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#todo-important-backpropagation-for-rnns","title":"{TODO} {IMPORTANT} Backpropagation for RNNs"},{"url":"#text-generation-rnns","title":"Text Generation RNNs"},{"url":"#note","title":"Note:"}]},"parent":{"__typename":"File","relativePath":"06_language_models_rnn.md"},"frontmatter":{"metaTitle":"This is the title tag of this page","metaDescription":"This is the meta description"}},"allMdx":{"edges":[{"node":{"fields":{"slug":"/00_toc","title":"Table of Contents"}}},{"node":{"fields":{"slug":"/03_word_window","title":"03. Word Window Classification, Neural Networks, and Matrix Calculus"}}},{"node":{"fields":{"slug":"/05_linguistic_structure_dependency_parsing","title":"05. Linguistic Structure: Dependency Parsing"}}},{"node":{"fields":{"slug":"/01_word_vectors","title":"Word Vectors"}}},{"node":{"fields":{"slug":"/06_language_models_rnn","title":"06. The probability of a sentence? Recurrent Neural Networks and Language Models"}}},{"node":{"fields":{"slug":"/15_natural_language_generation","title":"15. Natural Language Generation"}}},{"node":{"fields":{"slug":"/07_vanishing_gradients_fancy_rnn","title":"07. Vanishing Gradients and Fancy RNNs"}}},{"node":{"fields":{"slug":"/codeblock","title":"Syntax Highlighting"}}},{"node":{"fields":{"slug":"/extra_stuff","title":"Extra Stuff"}}},{"node":{"fields":{"slug":"/","title":"Welcome"}}},{"node":{"fields":{"slug":"/codeblock/1-index","title":"Sub Page"}}},{"node":{"fields":{"slug":"/01_word_vectors/02_assignment1","title":"Assignment 1"}}},{"node":{"fields":{"slug":"/01_word_vectors/03_word2vec","title":"Word2Vec"}}},{"node":{"fields":{"slug":"/01_word_vectors/01_count_based","title":"Count Based Word Vectors"}}},{"node":{"fields":{"slug":"/01_word_vectors/singular_value_decomposition","title":"Singular Value Decomposition"}}}]}},"pageContext":{"id":"5f99498f-8c16-57b1-9951-c2989bc4055d"}}}