{"data":{"allMdx":{"edges":[{"node":{"fields":{"slug":"/01_word_vectors"},"tableOfContents":{"items":[{"url":"#word-vectors","title":"Word Vectors:","items":[{"items":[{"url":"#word2vec-paper-suggested-2-methods","title":"Word2vec paper suggested 2 methods:"}]}]}]}}},{"node":{"fields":{"slug":"/05_linguistic_structure_dependency_parsing"},"tableOfContents":{"items":[{"url":"#lecture-plan-dependency-parsing","title":"Lecture plan: Dependency parsing"},{"url":"#why-do-we-need-sentence-structure","title":"Why do we need sentence structure?"},{"url":"#methods-of-dependency-parsing","title":"Methods of Dependency Parsing"}]}}},{"node":{"fields":{"slug":"/01_word_vectors/02_assignment1"},"tableOfContents":{}}},{"node":{"fields":{"slug":"/03_word_window"},"tableOfContents":{"items":[{"url":"#classification","title":"Classification:"},{"url":"#softmax-and-cross-entropy","title":"Softmax and cross entropy","items":[{"items":[{"url":"#step-1","title":"Step 1:"},{"url":"#step-2","title":"Step 2:"},{"url":"#step-3","title":"Step 3:"}]}]}]}}},{"node":{"fields":{"slug":"/07_vanishing_gradients_fancy_rnn"},"tableOfContents":{}}},{"node":{"fields":{"slug":"/codeblock"},"tableOfContents":{"items":[{"url":"#live-editing-example","title":"Live Editing example"}]}}},{"node":{"fields":{"slug":"/06_language_models_rnn"},"tableOfContents":{"items":[{"url":"#todo-important-backpropagation-for-rnns","title":"{TODO} {IMPORTANT} Backpropagation for RNNs"},{"url":"#text-generation-rnns","title":"Text Generation RNNs"},{"url":"#note","title":"Note:"}]}}},{"node":{"fields":{"slug":"/15_natural_language_generation"},"tableOfContents":{"items":[{"items":[{"url":"#plan","title":"Plan:","items":[{"url":"#what-is-nlg","title":"What is NLG:"}]},{"url":"#1-recap-what-we-already-know-about-nlg","title":"1. Recap what we already know about NLG","items":[{"url":"#recap-training-a-conditional-rnn-lm","title":"Recap: training a (conditional) RNN-LM:"},{"url":"#recap-decoding-algorithms","title":"Recap: decoding algorithms"},{"url":"#one-thing-not-discussed-earlier-about-beam-search","title":"One thing not discussed earlier about Beam Search:"},{"url":"#another-family-of-decoding-algorithms-sampling-based-decoding","title":"Another family of decoding algorithms: Sampling Based Decoding"},{"url":"#softmax-temperature","title":"Softmax temperature"},{"url":"#decoding-algorithms-in-summary","title":"Decoding algorithms: in summary"}]},{"url":"#2-nlg-tasks-and-neural-approaches-to-them","title":"2. NLG tasks and neural approaches to them","items":[{"url":"#211-summarization-task-definition","title":"2.1.1. Summarization: task definition"},{"url":"#212-summarization-how-to-do-it","title":"2.1.2. Summarization: How to do it","items":[{"url":"#explanation-about-pre-neural-way-of-summarization","title":"Explanation about pre-neural way of summarization:"}]},{"url":"#213-how-to-evaluate-summarization","title":"2.1.3. How to evaluate Summarization:"},{"url":"#214-neural-summarization-2015---present","title":"2.1.4. Neural summarization (2015 - present)"},{"url":"#215-one-solution-bottom-up-summarization","title":"2.1.5. One solution: bottom-up summarization:"}]}]},{"url":"#this-beginning-to-look-like-a-paper-tour","title":"This beginning to look like a paper tour","items":[{"items":[{"url":"#221-dialogue","title":"2.2.1. Dialogue"},{"url":"#222-seq2seq-based-dialogue","title":"2.2.2. Seq2seq-based dialogue"},{"url":"#223-negotiation-dialogue","title":"2.2.3. Negotiation dialogue"},{"url":"#231-storytelling","title":"2.3.1. Storytelling"},{"url":"#241-poetry-generation-hafez","title":"2.4.1. Poetry generation: Hafez"},{"url":"#251-non-autoregressive-generation-for-nmt","title":"2.5.1. Non-autoregressive generation for NMT"}]}]},{"url":"#thoughts-about-this-lecture","title":"Thoughts about this lecture","items":[{"url":"#3-nlg-evaluation","title":"3. NLG evaluation","items":[{"url":"#3xx-detailed-human-eval-of-controllable-chatbots","title":"3.x.x Detailed human eval of controllable chatbots"},{"url":"#possible-new-avenues-for-nlg-eval","title":"Possible new avenues for NLG eval?"}]},{"url":"#4-thoughts-on-nlg-research-current-trends-and-the-future","title":"4. Thoughts on NLG research, current trends, and the future","items":[{"url":"#41-exciting-current-trends-in-nlg","title":"4.1. Exciting current trends in NLG"},{"url":"#8-things-ive-learnt-from-working-in-nlg","title":"8 things Iâ€™ve learnt from working in NLG"}]}]}]}}},{"node":{"fields":{"slug":"/extra_stuff"},"tableOfContents":{"items":[{"url":"#actually-used","title":"Actually used:","items":[{"url":"#extra","title":"Extra:"}]}]}}},{"node":{"fields":{"slug":"/"},"tableOfContents":{"items":[{"url":"#aim-of-cs224n-course","title":"Aim of CS224n course:"},{"url":"#aims-of-this-tutorial","title":"Aims of this tutorial:"}]}}},{"node":{"fields":{"slug":"/codeblock/1-index"},"tableOfContents":{"items":[{"url":"#live-editing-example","title":"Live Editing example"}]}}},{"node":{"fields":{"slug":"/01_word_vectors/01_count_based"},"tableOfContents":{"items":[{"url":"#1-one-hot-vector","title":"1. One hot vector"},{"url":"#2-word-document-matrix","title":"2. Word document matrix"},{"url":"#3-window-based-co-occurence-matrix","title":"3. Window based co-occurence matrix"},{"url":"#4-tf-idf","title":"4. TF-IDF"}]}}},{"node":{"fields":{"slug":"/01_word_vectors/03_word2vec"},"tableOfContents":{"items":[{"url":"#1-one-hot-vector","title":"1. One hot vector"},{"url":"#2-word-document-matrix","title":"2. Word document matrix"},{"url":"#3-window-based-co-occurence-matrix","title":"3. Window based co-occurence matrix"},{"url":"#4-tf-idf","title":"4. TF-IDF"}]}}},{"node":{"fields":{"slug":"/00_toc"},"tableOfContents":{}}}]}}}